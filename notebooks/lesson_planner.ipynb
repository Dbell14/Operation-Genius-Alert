{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24020860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Casham2045\\miniconda3\\envs\\ds\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import pymupdf4llm\n",
    " \n",
    "import faiss\n",
    " \n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89a63991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dotenv in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (0.9.9)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from dotenv) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39bb7f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf_keras in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tf_keras) (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (72.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (1.74.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (3.11.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (0.5.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf_keras) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (14.1.0)\n",
      "Requirement already satisfied: namex in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (0.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf_keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf_keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf_keras) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf_keras) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf_keras) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf_keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf_keras) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf_keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (0.1.2)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (1.11.0.post1)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (4.55.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from sentence-transformers) (0.34.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from transformers) (0.6.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow) (5.29.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow) (72.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow) (1.74.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: rich in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from keras>=3.5.0->tensorflow) (14.1.0)\n",
      "Requirement already satisfied: namex in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Requirement already satisfied: pymupdf4llm in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (0.0.27)\n",
      "Requirement already satisfied: pymupdf>=1.26.3 in c:\\users\\casham2045\\miniconda3\\envs\\ds\\lib\\site-packages (from pymupdf4llm) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install tf_keras\n",
    "!pip install faiss-cpu sentence-transformers transformers tensorflow \n",
    "!pip install pymupdf4llm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "60b64348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tasks(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Finds a task for a student based on student's grade\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    \n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "    You are a Software Engineering tutor. You analyze students' grades and find short tasks to improve their learning. \n",
    "    Your job is to create clear, actionable 7-day study plans for students who are struggling in a subject. \n",
    "    Each plan should include:\n",
    "    - Daily micro-tasks (small, manageable steps)\n",
    "    - Spaced review of previously covered material\n",
    "    - Clear instructions for practice problems or study activities\n",
    "    - Motivational and encouraging language to help the student stay engaged\n",
    "    Keep the plan practical, concise, and easy for a student to follow. \n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"student's prompt is {text}    \n",
    "    \"\"\"\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "    )\n",
    "\n",
    "    output = completion.choices[0].message.content.strip()\n",
    "    print(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f516e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**7-Day Study Plan for Improving Your Understanding of For Loops**\n",
      "\n",
      "**Objective**: To build a strong foundation in using for loops through practice, review, and application.\n",
      "\n",
      "---\n",
      "\n",
      "### Day 1: Introduction to For Loops\n",
      "- **Micro-tasks**:\n",
      "  1. Watch a 10-minute introductory video on for loops (YouTube is a great resource).\n",
      "  2. Read a short article or tutorial that explains the syntax of for loops.\n",
      "- **Spaced Review**: Briefly review the basics of loops you might have learned in class (0-5 minutes).\n",
      "- **Practice Activity**: Write a simple for loop in your favorite programming language that prints numbers 1 to 10. \n",
      "- **Encouragement**: You're starting to break down barriers! Every bit of understanding will get you closer to mastering loops.\n",
      "\n",
      "---\n",
      "\n",
      "### Day 2: Basic Exercises\n",
      "- **Micro-tasks**:\n",
      "  1. Review the for loop syntax you learned yesterday (5 minutes).\n",
      "  2. Complete at least three basic exercises home assignments or online platforms (like LeetCode or HackerRank) focusing on for loops.\n",
      "- **Spaced Review**: Recall your notes from yesterday—check if you remember the syntax without looking.\n",
      "- **Practice Activity**: Modify your Day 1 loop to print \"Number: 1\" to \"Number: 10\" instead of just numbers.  \n",
      "- **Encouragement**: Small steps lead to big successes! You're doing great by mixing revision with new exercises.\n",
      "\n",
      "---\n",
      "\n",
      "### Day 3: Applying For Loops\n",
      "- **Micro-tasks**:\n",
      "  1. Find and read about common use cases for for loops in programming (10 minutes).\n",
      "  2. Choose a simple project idea (like generating a multiplication table) to work on.\n",
      "- **Spaced Review**: Review your Day 2 exercises. Try to solve at least one without looking at the solutions.\n",
      "- **Practice Activity**: Implement a multiplication table using for loops (1 to 10, for both sides). \n",
      "- **Encouragement**: You're making tangible progress! Projects help solidify concepts, and you're creating real code!\n",
      "\n",
      "---\n",
      "\n",
      "### Day 4: Nested For Loops\n",
      "- **Micro-tasks**:\n",
      "  1. Watch a short tutorial on nested for loops (10 minutes).\n",
      "  2. Read an article about when and why to use nested loops (5-10 minutes).\n",
      "- **Spaced Review**: Go over your multiplication table code from Day 3 and try to explain the logic behind it (5 minutes).\n",
      "- **Practice Activity**: Create a pattern with stars (e.g., a pyramid) using nested for loops.\n",
      "- **Encouragement**: You've reached the next level! Nested loops may seem complex, but you're more than capable of handling them.\n",
      "\n",
      "---\n",
      "\n",
      "### Day 5: Review and Reinforcement\n",
      "- **Micro-tasks**:\n",
      "  1. Review all your notes and code from the past four days (20 minutes).\n",
      "  2. Identify any areas where you still feel unsure.\n",
      "- **Spaced Review**: Quiz yourself on for loop syntax and common use cases (5 minutes).\n",
      "- **Practice Activity**: Solve additional problems that require both single and nested for loops (resources like freeCodeCamp may have relevant exercises).\n",
      "- **Encouragement**: Every review strengthens your knowledge! Remember, it’s okay to have questions; that’s how learning happens.\n",
      "\n",
      "---\n",
      "\n",
      "### Day 6: Advanced For Loop Concepts\n",
      "- **Micro-tasks**:\n",
      "  1. Watch a tutorial on how to work with collections (arrays, lists) using for loops (10 minutes).\n",
      "  2. Start researching for loop variations (like for-in, for-of for JS, or range in Python).\n",
      "- **Spaced Review**: Think through the advanced uses and types of for loops; discuss them with a peer or friend if possible (10 minutes).\n",
      "- **Practice Activity**: Create a program that takes an array of numbers and prints their squares.\n",
      "- **Encouragement**: Each day you are adding tools to your programming toolbox! Keep experimenting and pushing boundaries!\n",
      "\n",
      "---\n",
      "\n",
      "### Day 7: Reflection and Application\n",
      "- **Micro-tasks**:\n",
      "  1. Reflect on what you’ve learned over the week. Write down key concepts (10 minutes).\n",
      "  2. Map out a mini project where you can effectively use for loops.\n",
      "- **Spaced Review**: Go over your previous exercises and refine your code, applying any new knowledge (15 minutes).\n",
      "- **Practice Activity**: Implement your mini project idea from the previous task. \n",
      "- **Encouragement**: You’ve come full circle! Every coder struggles at times, but you’ve put in the work—be proud of your progress!\n",
      "\n",
      "---\n",
      "\n",
      "### Final Note:\n",
      "Keep practicing beyond this week! For loops are a fundamental part of programming; the more you use them, the more comfortable you’ll become. Celebrate your achievements, and know that your hard work will pay off! 🎉\n",
      "**7-Day Study Plan for Improving Your Understanding of For Loops**\n",
      "\n",
      "**Objective**: To build a strong foundation in using for loops through practice, review, and application.\n",
      "\n",
      "---\n",
      "\n",
      "### Day 1: Introduction to For Loops\n",
      "- **Micro-tasks**:\n",
      "  1. Watch a 10-minute introductory video on for loops (YouTube is a great resource).\n",
      "  2. Read a short article or tutorial that explains the syntax of for loops.\n",
      "- **Spaced Review**: Briefly review the basics of loops you might have learned in class (0-5 minutes).\n",
      "- **Practice Activity**: Write a simple for loop in your favorite programming language that prints numbers 1 to 10. \n",
      "- **Encouragement**: You're starting to break down barriers! Every bit of understanding will get you closer to mastering loops.\n",
      "\n",
      "---\n",
      "\n",
      "### Day 2: Basic Exercises\n",
      "- **Micro-tasks**:\n",
      "  1. Review the for loop syntax you learned yesterday (5 minutes).\n",
      "  2. Complete at least three basic exercises home assignments or online platforms (like LeetCode or HackerRank) focusing on for loops.\n",
      "- **Spaced Review**: Recall your notes from yesterday—check if you remember the syntax without looking.\n",
      "- **Practice Activity**: Modify your Day 1 loop to print \"Number: 1\" to \"Number: 10\" instead of just numbers.  \n",
      "- **Encouragement**: Small steps lead to big successes! You're doing great by mixing revision with new exercises.\n",
      "\n",
      "---\n",
      "\n",
      "### Day 3: Applying For Loops\n",
      "- **Micro-tasks**:\n",
      "  1. Find and read about common use cases for for loops in programming (10 minutes).\n",
      "  2. Choose a simple project idea (like generating a multiplication table) to work on.\n",
      "- **Spaced Review**: Review your Day 2 exercises. Try to solve at least one without looking at the solutions.\n",
      "- **Practice Activity**: Implement a multiplication table using for loops (1 to 10, for both sides). \n",
      "- **Encouragement**: You're making tangible progress! Projects help solidify concepts, and you're creating real code!\n",
      "\n",
      "---\n",
      "\n",
      "### Day 4: Nested For Loops\n",
      "- **Micro-tasks**:\n",
      "  1. Watch a short tutorial on nested for loops (10 minutes).\n",
      "  2. Read an article about when and why to use nested loops (5-10 minutes).\n",
      "- **Spaced Review**: Go over your multiplication table code from Day 3 and try to explain the logic behind it (5 minutes).\n",
      "- **Practice Activity**: Create a pattern with stars (e.g., a pyramid) using nested for loops.\n",
      "- **Encouragement**: You've reached the next level! Nested loops may seem complex, but you're more than capable of handling them.\n",
      "\n",
      "---\n",
      "\n",
      "### Day 5: Review and Reinforcement\n",
      "- **Micro-tasks**:\n",
      "  1. Review all your notes and code from the past four days (20 minutes).\n",
      "  2. Identify any areas where you still feel unsure.\n",
      "- **Spaced Review**: Quiz yourself on for loop syntax and common use cases (5 minutes).\n",
      "- **Practice Activity**: Solve additional problems that require both single and nested for loops (resources like freeCodeCamp may have relevant exercises).\n",
      "- **Encouragement**: Every review strengthens your knowledge! Remember, it’s okay to have questions; that’s how learning happens.\n",
      "\n",
      "---\n",
      "\n",
      "### Day 6: Advanced For Loop Concepts\n",
      "- **Micro-tasks**:\n",
      "  1. Watch a tutorial on how to work with collections (arrays, lists) using for loops (10 minutes).\n",
      "  2. Start researching for loop variations (like for-in, for-of for JS, or range in Python).\n",
      "- **Spaced Review**: Think through the advanced uses and types of for loops; discuss them with a peer or friend if possible (10 minutes).\n",
      "- **Practice Activity**: Create a program that takes an array of numbers and prints their squares.\n",
      "- **Encouragement**: Each day you are adding tools to your programming toolbox! Keep experimenting and pushing boundaries!\n",
      "\n",
      "---\n",
      "\n",
      "### Day 7: Reflection and Application\n",
      "- **Micro-tasks**:\n",
      "  1. Reflect on what you’ve learned over the week. Write down key concepts (10 minutes).\n",
      "  2. Map out a mini project where you can effectively use for loops.\n",
      "- **Spaced Review**: Go over your previous exercises and refine your code, applying any new knowledge (15 minutes).\n",
      "- **Practice Activity**: Implement your mini project idea from the previous task. \n",
      "- **Encouragement**: You’ve come full circle! Every coder struggles at times, but you’ve put in the work—be proud of your progress!\n",
      "\n",
      "---\n",
      "\n",
      "### Final Note:\n",
      "Keep practicing beyond this week! For loops are a fundamental part of programming; the more you use them, the more comfortable you’ll become. Celebrate your achievements, and know that your hard work will pay off! 🎉\n"
     ]
    }
   ],
   "source": [
    "response = get_tasks('I got a low grade in for loops')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d75c066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "data = \".\"\n",
    "def get_tags(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Finds tags for the requested topic in software engineering\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "    Can you come up with a list of tags to quickly search material for the 7 day study plan for software engineers. \n",
    "    These are the topics covered in the Data Science course:{data}\n",
    "    Don't use hashtag in the beginning of the words. Don't use hashtags from social media.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"student's prompt is {text}    \n",
    "    \"\"\"\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "    )\n",
    "\n",
    "    output = completion.choices[0].message.content.strip()\n",
    "    #print(output)\n",
    "    return output\n",
    "\n",
    "get_tags(\"AB Testing\")\n",
    "updated_tags = get_tags('AB Testing')\n",
    "print(type(updated_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5b20ae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_tags = updated_tags.replace(\"\\n\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ab13bfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Introduction to K-Nearest-Neighbors.pdf:\n",
      "# **Introduction to** **K-Nearest-Neighbors**\n",
      "\n",
      "\n",
      "#### **Agenda - Goals**\n",
      "\n",
      "##### **●**\n",
      "\n",
      "\n",
      "## **Warm Up**\n",
      "\n",
      "\n",
      "## **k-Nearest Neighbors**\n",
      "\n",
      "\n",
      "#### **k-Nearest Neighbors (kNN)**\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#### **k-Nearest Neighbors (\n",
      "Processing NLP & Vector Embeddings.pdf:\n",
      "# **Natural Language** **Processing & Vector** **Embeddings**\n",
      "\n",
      "\n",
      "### **Agenda - Goals**\n",
      "\n",
      "\n",
      "**●** **…**\n",
      "\n",
      "\n",
      "## **Kahoot**\n",
      "\n",
      "\n",
      "Let’s begin todays Kahoot\n",
      "\n",
      "\n",
      "## **Pre-Class Review**\n",
      "\n",
      "\n",
      "### **Pre-Class Review**\n",
      "\n",
      "\n",
      "\n",
      "Processing Probability Review.pdf:\n",
      "# **Probability Review**\n",
      "\n",
      "\n",
      "#### **Agenda - Schedule**\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#### **Agenda - Announcements**\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#### **Agenda - Goals**\n",
      "\n",
      "\n",
      "**●**\n",
      "**Review basic probability**\n",
      "\n",
      "\n",
      "**●**\n",
      "**Understand how sample calculation\n",
      "Processing Random Forests.pdf:\n",
      "# **Random Forests &** **Boosted Forests**\n",
      "\n",
      "\n",
      "### **Agenda - Schedule**\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### **Agenda - Goals**\n",
      "\n",
      "\n",
      "**●**\n",
      "\n",
      "\n",
      "## **Bagging**\n",
      "\n",
      "\n",
      "|team1_3point|team2_turnover|team1_won|\n",
      "|---|---|---|\n",
      "|35|15|1|\n",
      "|28.5|12|\n",
      "Processing Shopping Dataset Case Study.pdf:\n",
      "# **Shopping Dataset Case** **Study**\n",
      "\n",
      "\n",
      "### **Agenda - Goals**\n",
      "\n",
      "\n",
      " Apply basic and intermediate pandas methods to **explore a structured dataset**\n",
      "\n",
      "\n",
      " - Perform **univariate and bivariate analysis** on \n",
      "Processing SQL Review II.pdf:\n",
      "# **SQL Review II**\n",
      "\n",
      "\n",
      "### **Agenda - Announcements**\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### **Agenda - Goals**\n",
      "\n",
      "\n",
      "**●**\n",
      "**Simulate the interview process by asking you to complete and present**\n",
      "\n",
      "\n",
      "**technical challenges live.**\n",
      "\n",
      "\n",
      "**●*\n",
      "Processing SQL Review.pdf:\n",
      "# **SQL Review**\n",
      "\n",
      "\n",
      "### **Agenda - Announcements**\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "### **Agenda - Goals**\n",
      "\n",
      "\n",
      "**●**\n",
      "**Simulate the interview process by asking you to complete and present**\n",
      "\n",
      "\n",
      "**technical challenges live.**\n",
      "\n",
      "\n",
      "**●**\n",
      "*\n",
      "Processing Transformer Architecture.pdf:\n",
      "# **A Quick Run-Through of** **Transformer** **Architecture**\n",
      "\n",
      "\n",
      "### **Agenda - Goals**\n",
      "\n",
      "#### **●** **Understand how LLMs work at a high level** **●** **Big idea behind Attention** **●** **Why Attentio\n",
      "Processing Twitter Dataset Case Study.pptx.pdf:\n",
      "# **Twitter Dataset Case** **Study**\n",
      "\n",
      "\n",
      "#### **Agenda - Goals**\n",
      "\n",
      "\n",
      " Perform time series exploration using pandas\n",
      "\n",
      "\n",
      " Extract key date-time features like hour and weekday\n",
      "\n",
      "\n",
      " Analyze and visualize patterns\n",
      "Processing Types of Visualizations Review.pdf:\n",
      "# **Types of Visualizations** **Review**\n",
      "\n",
      "\n",
      "|“The existence of Comet NEOWISE (here depicted as a series|Col2|\n",
      "|---|---|\n",
      "|_of red dots) was discovered by analyzing astronomical survey_|_of red dots) was\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "slides = {}\n",
    "pdf_directory = r\"../data\"\n",
    "for filename in os.listdir(pdf_directory):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            filepath = os.path.join(pdf_directory, filename)\n",
    "            # Now, you can process each PDF file using pymupdf4llm\n",
    "            # For example, to extract text as Markdown:\n",
    "            markdown_content = pymupdf4llm.to_markdown(filepath)\n",
    "            print(f\"Processing {filename}:\")\n",
    "            # You can then work with the extracted markdown_content\n",
    "            print(markdown_content[:200]) # Print first 200 characters for demonstration\n",
    "            slides[filename] = markdown_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e4271760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Introduction to K-Nearest-Neighbors.pdf': '# **Introduction to** **K-Nearest-Neighbors**#### **Agenda - Goals**##### **●**## **Warm Up**## **k-Nearest Neighbors**#### **k-Nearest Neighbors (kNN)**#### **k-Nearest Neighbors (kNN)**##### You recognize some cybersecurity fellows, as well as data science fellows. **You notice that everyone is congregating and chatting in groups.**##### But then, you notice someone that you do not recognize. Considering however that all cyber-security fellows are sticking together, which **fellowship does this person most likely belong to?**##### Most likely a cyber-security fellow, no?##### What if we have someone that’s on the periphery of both groups ? We’ll solve this during our discussion of the kNN algorithm.#### **k-Nearest Neighbors (kNN)**We formalize this idea via the following formula for conditional probability. Once again let’s break thisdown before moving to an example.The likelihood the **class of this sample is “j”,** given **the point x** **0** is equal to*Note, we identify the set of “K” nearest points as N 0 . There’snothing special about this name (for now), we could have named it“Roofus” if we wanted to.The likelihood the **class of this sample is “j”,** given **the point x** **0** is equal toThe likelihood the **class of this sample is “j”,** given **the point x** **0** is equal toThe likelihood the **class of this sample is “j”,** given **the point x** **0** is equal toYou may be thinking “ _Darn that’s a whole lot of symbols to say something so simple_ .”However, when it comes to creating ideas in maths, we **cannot be ambiguous** . Therefore weformalize these ideas in a **common set of symbols that all humans agree on** . It’s a beautifulthing really.Succinctly put, we estimate the **conditional probability that x0 belongs to class j as the fraction of the K****nearest points** **whose response values equal j** .Let’s bring back our kibble/loud dataset that identifies **cats, hamsters, and dogs** . I reduced thedata-points to make this graph easier to discuss. You may notice the axes now have equal range, this willI introduce a mystery animal. Let’s assume they are located on coordinate (40, 30). Let’s estimate theprobability that this animal is a cat, dog, or hamster using our **estimated conditional probability.**#### **Small Aside - Euclidean Distance (L2)**#### **Small Aside - Euclidean Distance (L2)**#### **Small Aside - Euclidean Distance (L2)**#### **Small Aside - Euclidean Distance (L2)**#### **Small Aside - Euclidean Distance (L2)**#### **Small Aside - Euclidean Distance (L2)**#### **Small Aside - Euclidean Distance (L2)**#### **Small Aside - Manhattan Distance (L1)**Now, picture we can only travel in straight lines andturn in right angles (as if we were on the streets of**manhattan** ).This entails simply adding the sides of our righttriangle!Can anyone calculate this?#### **Small Aside - Manhattan Distance (L1)**Now, picture we can only travel in straight lines andturn in right angles (as if we were on the streets of**manhattan** ).This entails simply adding the sides of our righttriangle!Can anyone calculate this?And perhaps propose a formula for us to use?#### **Small Aside - Manhattan Distance (L1)**##### ⅔ ⅓We’re going to pause our discussion ofkNN here.However there is more to evaluate whendiscussing this **simple, yet powerful****algorithm.**What are some questions you mighthave regarding kNN?We’re going to pause our discussion ofkNN here.However there is more to evaluate whendiscussing this **simple, yet powerful****algorithm.**What are some questions you mighthave regarding kNN?**●****Why are my axes normalized?****●** **When do we use euclidean vs****manhattan?****●****What happens if we have****categorical data?****●****What happens as I increase my****dimensions?****●** **What are some cons to kNN?**##### Going back to our TKH industry event discussion, we simply select some “K” that accurately classifies this person as a data or cyber fellow.##### **k=3**##### **Class = Cyber**##### **k=5**##### **Class = Cyber**##### Class = Cyber ( are we suffering from imbalance??? )## **Choosing a K - HyperParam**##### Let’s bring back our scatter plot (with a twist!) To obtain the decision boundary for kNN, we could compute the class using kNN for eachRed = cat; yellow = hammie; blue = dog##### We begin with k=1. What do you notice about the error rate? Does it make any mistakes?Red = cat; yellow = hammie; blue = dog##### None whatsoever! While our “training” rate might seem ideal, let’s see what happens when we introduce a new test sample. Who is our hamster friend **classified as?**##### A cat . Remember everyone, a classifier that perfectly fits our training dataRed = cat; yellow = hammie; blue = dog##### We move onto k=3. What happens to our training error rate now?Red = cat; yellow = hammie; blue = dog##### Unfortunately a few training samples are classified incorrectly, but this could actually be preferable when it comes to our testing data !##### Bringing back our hamster, we see that it is now correctly classified as a hamster (ignore the roughness of my sketches)Red = cat; yellow = hammie; blue = dog##### Finally lets try k=6. What happens to our training error rate now?#### **kNN - Importance of Data Scaling**##### _X1: (-2 to 2)_ _X2: (-2 to 2)_ Now that we’ve standardized our dataset, we can assign equal **importance to dimensions.** Subsequently, we update our predictions.#### **kNN**###### To conclude our conversation on the supervised learning classifier kNN, it is a **powerful non-parametric supervised learning algorithm that utilizes a** . **fairly “lazy” classification method** Pros ● No optimization involved ● Comparable performance to Naive Bayes **●** **Easy to understand** Cons ● Sensitive to class imbalance ● Need to store entire training dataset for prediction                       Good visual of kNN from https://knn [notebook.netlify.app/](https://knn-notebook.netlify.app/)##### Let’s review the following gif to see the process that kNN takes to classify a new test observation.## **Classification Challenges**#### **Classification Challenges**#### **Choosing a Distance Metric**#### **Choosing a Distance Metric**What do you notice about the **sparsity** of our data as we increase dimensions?Notice how the frequency of distances converges on a specific value as we increase dimensions:[https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote02_kNN.html](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote02_kNN.html)#### **The Curse of Dimensionality**##### Lower dimensional manifold in a nutshell: “ Wow this looks really complex… ”#### **Handling Data Imbalance**_True Positives + True Negatives__False Negatives_##### This leads to what we call the **accuracy** paradox (null accuracy). Let’s say you go to another industry event and you see 9 cyber fellows and 1 data fellow. You train a kNN algorithm and it gives you a 90% accuracy rate. Wow! This must be a **good model, no?**_True Positives + True Negatives__False Negatives__9 + 0_**90%**_9 + 0_**90%**#### **Over-Sampling Vs Under-Sampling**#### **Over-Sampling Vs Under-Sampling**#### **SMOTE**_Synthetic Minority Over-sampling Technique_This technique entails creating new _synthetic minority classes_ via a somewhat randomized process.1. Choose a random set of the minority class2. Find k-nearest-neighbors for each sample of the same class3. Calculate a new synthetic class between this sample and all of its neighborsa. Choose a random value between 0 & 1 to calculate where this newly random point willexist in the space between your data point and its neighbors##### This allows us to “purposefully” oversample our dataset to improve the predictive qualities of our dataset.#### **Handling Data Imbalance**##### This is especially useful when trying to detect some real relatively rare class in your dataset: ● Actual fraud **● A rare disease** ● **A 16-seeded team beating a 1-seeded team**## **End of Class Announcements**##### ○ Where does variance/bias exist in kNN?|Neural network training|makes||---|---||**_beautiful fractals_**||',\n",
       " 'NLP & Vector Embeddings.pdf': '# **Natural Language** **Processing & Vector** **Embeddings**### **Agenda - Goals****●** **…**## **Kahoot**Let’s begin todays Kahoot## **Pre-Class Review**### **Pre-Class Review**## **Natural Language Processing**### **Natural Language Processing**### **Natural Language Processing Techniques**As it turns out, computer scientists have been attempting to solve this problemfor a quite a long time. Let’s review the foundational techniques of analyzing a“corpus” of text._**●**__**Bag of Words**_ (Harris 1954)_**●**_ _**TF-IDF**_ (Jones 1972)These techniques are dated by this point, **but it doesn’t mean these techniques****are useless.** Let’s see what their purpose is in the world of document analysis.### **Bag of Words**Let’s take three example text files and see how the BOW technique analyzes them.We will use **3 example reviews from Amazon and their subsequent reviews (out****of 5 stars)** .What’s the first step to the BOW technique?**(1)****(2)**### **Term Frequency-Inverse Document Frequency** **(TF-IDF)**While BOW is a trivial technique to interpret text, we can iterate upon thisimplementation via the **Term Frequency-Inverse Document Frequency** algorithm.The steps to this include:_**1.**_ _**BOW steps up to step 2**__**2.**_ _**Calculate the total number of times word t appears in document d, divided by all the**__**terms in document d (term frequency)**__**3.**_ _**Calculate the total number of documents divided by the number of documents**__**containing term t (inverse document frequency)**__**4.**_ _**Multiply these two values together to calculate the tf-idf.**_## **Vector Embeddings**### **Word Embeddings - Data Representations**### **Word Embeddings - Word2Vec**By observing wheres these data-samples exist in the **semantic feature**Lastly, to demonstrate the power of word2vec transformations, let’s say wehave the additional data-samples “ **evening** ” and “ **dinner** .”Try adding the vectors “lunch” + “evening” [1, 4] + [3, -3].Which value do we get?### **Word Embeddings - Word2Vec**### **Word Embeddings - Word2Vec**### **Word Embeddings - Word2Vec (SkipGram)**Let’s observe the steps to train a **skip-gram word2vec**_**1.**_ _**Tokenize your text into words (or n-grams)**__**2.**_ _**Eliminate stop-words and punctuation**_**3.** **Choose a window size of “n”****4.** **Loop through each window of your corpus****a.****Train your single hidden layer neural network on each (n) context****words & target word**_**The dog eats the bone**__**The cat eats fish.**__**The cat scratches its owner.**_## **Embedding Everything**### **Transformer Embeddings**## **End of Class Announcements**',\n",
       " 'Probability Review.pdf': '# **Probability Review**#### **Agenda - Schedule**#### **Agenda - Announcements**#### **Agenda - Goals****●****Review basic probability****●****Understand how sample calculations differ from population****●****Understand & apply Bayes Theorem**## **Inferring from a Sample**We often use humans when discussing population vs sample, but thisapplies **to any dataset** . Ex: All AAPL stock prices since Dec 14, 1984( **population** ) & AAPL stock price on Dec 12, 2024 ( **sample** ).Our equations for measures of dispersion change based on which subset weare calculating our metrics on. This is **because a sample is an estimate**, and a.**population is ground truth**## **Probability Review**|Event|Probability||---|---||**Dog**|0.5||**Cat**|0.3||**Hamster**|0.1||**Lizard**|0.0||**No pet**|0.1|Let’s increase our sample space in order to discuss more complex terms.Here we describe the probabilities that an american household has somespecific pet.**Dog** **Cat** **Hamster** **Lizard** **None**Graphing these values gives us the **probability distribution** .|Event|Probability||---|---||**Dog**|0.5||**Cat**|0.3||**Hamster**|0.1||**Lizard**|0.0||**No pet**|0.1|## **Probability in Data Analysis -** **Bayes Theorem**#### **Probability in Data Analysis**#### **Bayes Theorem - Frequentist World-View**#### **Bayes Theorem - Frequentist Pros**#### **Bayes Theorem - Frequentist Cons**#### **Bayes Theorem - Frequentist Cons**#### **Bayes Theorem - Bayesian Statistics**_statistician_## **P(H) P(E|H)**Here’s the formula, let’s **break down its components** before going throughour covid example.Remember, the “|” is the symbolfor conditional probabilitystatements.“Hypothesis given Event”## **P(H) P(E|H)**The probability the **hypothesis** is true given the **event** is equal toThe probability the **hypothesis** is true given the **event** is equal toThe probability the **hypothesis** is true given the **event** is equal to## **P(H) P(E|H)**Divided by the probability of **event** occurring. This part is sometimesdeceptively simple. Keep in mind that we want to consider the **event occurs****given the hypothesis is true and given the hypothesis is false** !Let’s understand how to calculate this. I find diagrams to be the most helpfulin visualizing this calculationTo calculate P(E), we have to consider the intersection of P(E) and P(H), aswell as the space where P(E) exists outside of P(H)This intersect can be labeled **P(E & H)**In terms of conditional probability, that is **P(H)P(E|H)**And lastly we have **P(E & -H).** Can anyone express this in terms ofconditional probability as well?**P(-H)P(E|-H). This allows us to express P(E) as P(H)P(E|H) + P(-H)P(E|-H)**## **P(H) P(E|H)****P(H|E) = 0.089****P(H|E) = 0.47**## **Wrap-Up**',\n",
       " 'Random Forests.pdf': '# **Random Forests &** **Boosted Forests**### **Agenda - Schedule**### **Agenda - Goals****●**## **Bagging**|team1_3point|team2_turnover|team1_won||---|---|---||35|15|1||28.5|12|0||40|18|1||25.0|20.0|0|1 0 1 0And this is only for our4-sample dataset. Whatif we have 100, 1000 or1,000,000 samples?|team1_3point|team2_turnover|team1_won||---|---|---||35|15|1||28.5|12|0||40|18|1||25.0|20.0|0|Going back to the formulation of our decision trees, our trees will **overfit to****this noisy data and fail to generalize on new test samples.**Is this considered high bias or high variance?1 0 1 0|team1_3point|team2_turnover|team1_won||---|---|---||35|15|1||28.5|12|0||40|18|1||25.0|20.0|0|Therefore, we need a method to cut through all this noise and somehow**extract the true signal from this dataset** . The first technique we will learn is.**bagging**### **Bagging - Bootstrapping**### **Bagging - Bootstrapping**### **Bagging - Bootstrapping + Aggregating**Let’s apply this technique to our decision trees inorder to implement what we call “ **bagged trees.** ”The algorithm for **bagging (bootstrapping +****aggregating)** is applied through the following steps:1. Select _n_ random subsets of datasets withreplacement ( **bootstrap** )2. Grow arbitrarily large trees on each subset3. “ _Democratically_ ” select the prediction whichhas the most votes across all trees( **aggregate** ).|team1_3point|team2_turnover|team1_won||---|---|---||35|15|1||28.5|12|0||40|18|1||25.0|20.0|0||37.5|14.0|1||30.0|16.0|0||42.0|19.5|1||33.5|21.0|0|Let’s bring back our basketball dataset to see how we can apply **bagging** andstrengthen the true signal of our dataset. We include a few more data pointsto make this a little more interesting.|28.5|12|0||---|---|---||37.5|14.0|1||30.0|16.0|0||Col1|35|15|1||---|---|---|---|||42.0|19.5|1|||28.5|12|0||40|18|1||---|---|---||33.5|21.0|0||25.0|20.0|0||team1_3point|team2_turnover|team1_won||---|---|---||35|15|1||28.5|12|0||40|18|1||25.0|20.0|0||37.5|14.0|1||30.0|16.0|0||42.0|19.5|1||33.5|21.0|0||Col1|28.5|12|0||---|---|---|---|||42.0|19.5|1|||28.5|12|0|First, we will make 4 random subsets of data (with replacement). This is the**bootstrap** step.**Note** : keep in mind thatthis example is toosimple to make anymeaningful tree|28.5|12|0||---|---|---||37.5|14.0|1||30.0|16.0|0|1 0|35|15|1||---|---|---||42.0|19.5|1||28.5|12|0||40|18|1||---|---|---||33.5|21.0|0||25.0|20.0|0||28.5|12|0||---|---|---||42.0|19.5|1||28.5|12|0|1 0|team1_3point|team2_turnover|team1_won_pred||---|---|---||35|15|???||28.5|12|???||40|18|???||25.0|20.0|???||37.5|14.0|???||30.0|16.0|???||42.0|19.5|???||33.5|21.0|???|1 0|team1_3point|team2_turnover|team1_won_pred||---|---|---||35|15|???||28.5|12|???||40|18|???||25.0|20.0|???||37.5|14.0|???||30.0|16.0|???||42.0|19.5|???||33.5|21.0|???|1 010|team1_3point|team2_turnover|team1_won_pred||---|---|---||35|15|???||28.5|12|???||40|18|???||25.0|20.0|???||37.5|14.0|???||30.0|16.0|???||42.0|19.5|???||33.5|21.0|???|One question that youmight be thinking is“wait so now we’veintroduced a parameter’to indicate how manytrees we will train, isthis anotherhyperparameter thatwe must find viaGridSearch?”1 0101 0|team1_3point|team2_turnover|team1_won_pred||---|---|---||35|15|**1**||28.5|12|???||40|18|???||25.0|20.0|???||37.5|14.0|???||30.0|16.0|???||42.0|19.5|???||33.5|21.0|???|1 01 0tie|team1_3point|team2_turnover|team1_won_pred|team1_won||---|---|---|---||35|15|**1**|1||28.5|12|**0**|0||40|18|**1**|1||25.0|20.0|**0**|0||37.5|14.0|**1**|1||30.0|16.0|**0**|0||42.0|19.5|**1**|1||33.5|21.0|**0 or 1**|0|Just for fun, let’s fill out the rest of these samples and observe the accuracy. Noticethat we occasionally get **ties.** In this case, we decide randomly which class tochoose which leads us to an accuracy that ranges from **85% to 100%**As it turns out, the number of trees that we train _**“B”**_ does not suffer from overfitting (high variance) as weincrease this amount! Instead, error simply _settles_ after enough trees are generated. **WOW!**Therefore, we simplyuse a **sufficiently large****B** until we do not seelarge gains in testaccuracy.One technique we canuse is to start large(100), and then see howmany trees we can**remove for the same****test accuracy rate.**Impurity Removed = 0.8751 0Impurity Removed = 0.33Impurity Removed = 11 0Impurity Removed = 0.8751 0By adding all the impurity removed, we can observe which predictor is the **most important** . In thisdataset, which feature seems to remove the most impurity?## **Random Forests**### **Random Forests**|team1_3point|team2_turnover|team1_won||---|---|---||35|15|1||28.5|12|0||40|18|1||25.0|20.0|0||37.5|14.0|1||30.0|16.0|0||42.0|19.5|1||33.5|21.0|0|Let’s go back to our basketball example once more, and walk through thesteps of bagging with this additional caveat|28.5|12|0||---|---|---||37.5|14.0|1||30.0|16.0|0||Col1|35|15|1||---|---|---|---|||42.0|19.5|1|||28.5|12|0||40|18|1||---|---|---||33.5|21.0|0||25.0|20.0|0||team1_3point|team2_turnover|team1_won||---|---|---||35|15|1||28.5|12|0||40|18|1||25.0|20.0|0||37.5|14.0|1||30.0|16.0|0||42.0|19.5|1||33.5|21.0|0||Col1|28.5|12|0||---|---|---|---|||42.0|19.5|1|||28.5|12|0|First, we will make 4 random subsets of data (with replacement). This is the**bootstrap** step.|28.5|12|0||---|---|---||37.5|14.0|1||30.0|16.0|0||35|15|1||---|---|---||42.0|19.5|1||28.5|12|0||40|18|1||---|---|---||33.5|21.0|0||25.0|20.0|0||28.5|12|0||---|---|---||42.0|19.5|1||28.5|12|0|1 01 0|team1_3point|team2_turnover|team1_won_pred|team1_won||---|---|---|---||35|15|**1**|1||28.5|12|**0**|0||40|18|**1**|1||25.0|20.0|**0**|0||37.5|14.0|**1**|1||30.0|16.0|**0**|0||42.0|19.5|**1**|1||33.5|21.0|**1**|0|Once again, we allow these trees to democratically elect the prediction based on their **bootstrapped****samples and limited predictors. Notice that we get no ties! We walk away with one accuracy!**This outcome is not unique to our dataset. Notice the **Test:RandomForest** error rate performs noticeably**better than the simple bagged model** .- Computationally **expensive**## **Boosting - AdaBoost****Trees with better information on****which mistakes they should avoid****should also be better classifiers!****Metaphors get tough in this context****but consider the following:****You, a New Yorker/Connecticuter,****are voting for public transportation****policies in your city. Might you have****better context than an extra-state****voter?**|28.5|12|0||---|---|---||37.5|14.0|1||30.0|16.0|0|1 0|35|15|1||---|---|---||42.0|19.5|1||28.5|12|0||40|18|1||---|---|---||33.5|21.0|0||25.0|20.0|0||28.5|12|0||---|---|---||42.0|19.5|1||28.5|12|0|### **Boosting - AdaBoost**|team1_3point|team2_turnover|team1_won|sample weight||---|---|---|---||35|15|1|1/8||28.5|12|0|1/8||40|18|1|1/8||25.0|20.0|0|1/8||37.5|14.0|1|1/8||30.0|16.0|0|1/8||42.0|19.5|1|1/8||33.5|21.0|0|1/8|1 0|team1_3point|team2_turnover|team1_won|sample weight||---|---|---|---||35|15|1|1/8||28.5|12|0|1/8||40|18|1|1/8||25.0|20.0|0|1/8||37.5|14.0|1|1/8||30.0|16.0|0|1/8||42.0|19.5|1|1/8||33.5|21.0|0|1/8|Just like with random forests, we will grow **stumps** on a subset of predictors(one tree per predictor). However this time we use the **entire dataset.**= 0= 2= 4= 2Gini = 1 ((0/2)^2+(2/2)^2)= 2= 2= 2= 2|team1_3point|team2_turnover|team1_won|sample weight||---|---|---|---||35|15|1|1/8||28.5|12|0|1/8||40|18|1|1/8||25.0|20.0|0|1/8||37.5|14.0|1|1/8||30.0|16.0|0|1/8||42.0|19.5|1|1/8||33.5|21.0|0|1/8|= 0= 2= 4= 2Gini = 0.44Gini = 0= 2= 2= 2= 2|team1_3point|team2_turnover|team1_won|sample weight||---|---|---|---||35|15|1|1/8||28.5|12|0|1/8||40|18|1|1/8||25.0|20.0|0|1/8||37.5|14.0|1|1/8||30.0|16.0|0|1/8||42.0|19.5|1|1/8||33.5|21.0|0|1/8|= 0= 2= 4= 2Total Gini = 0.44 *(6/8) + 0 * (2/8)= 2= 2= 2= 2|team1_3point|team2_turnover|team1_won|sample weight||---|---|---|---||35|15|1|1/8||28.5|12|0|1/8||40|18|1|1/8||25.0|20.0|0|1/8||37.5|14.0|1|1/8||30.0|16.0|0|1/8||42.0|19.5|1|1/8||33.5|21.0|0|1/8|= 0= 2= 4= 2= 2= 2= 2= 2|team1_3point|team2_turnover|team1_won|sample weight||---|---|---|---||35|15|1|1/8||28.5|12|0|1/8||40|18|1|1/8||25.0|20.0|0|1/8||37.5|14.0|1|1/8||30.0|16.0|0|1/8||42.0|19.5|1|1/8||33.5|21.0|0|1/8|= 0= 2= 4= 2Total Error = ⅛ + ⅛ = **0.25**0 -> Perfect stump1 -> Bad stump|team1_3point|team2_turnover|team1_won|sample weight||---|---|---|---||35|15|1|1/8||28.5|12|0|1/8||40|18|1|1/8||25.0|20.0|0|1/8||37.5|14.0|1|1/8||30.0|16.0|0|1/8||42.0|19.5|1|1/8||33.5|21.0|0|1/8|= 0= 2= 4= 2|team1_3point|team2_turnover|team1_won|sample weight||---|---|---|---||35|15|1|1/8||28.5|12|0|1/8||40|18|1|1/8||25.0|20.0|0|1/8||37.5|14.0|1|1/8||30.0|16.0|0|1/8||42.0|19.5|1|1/8||33.5|21.0|0|1/8||team1_3point|team2_turnover|team1_won|sample weight||---|---|---|---||35|15|1|1/8||28.5|12|0|1/8||40|18|1|1/8||25.0|20.0|0|1/8||37.5|14.0|1|1/8||30.0|16.0|0|1/8||42.0|19.5|1|1/8||33.5|21.0|0|1/8||team1_3point|team2_turnover|team1_won|sample weight||---|---|---|---||35|15|1|1/8||28.5|12|0|1/8||40|18|1|1/8||25.0|20.0|0|1/8||37.5|14.0|1|1/8||30.0|16.0|0|1/8||42.0|19.5|1|1/8||33.5|21.0|0|1/8||team1_3point|team2_turnover|team1_won|sample weight||---|---|---|---||35|15|1|0.10||28.5|12|0|0.10||40|18|1|0.10||25.0|20.0|0|0.10||37.5|14.0|1|0.10||30.0|16.0|0|0.16||42.0|19.5|1|0.10||33.5|21.0|0|0.16||team1_3point|team2_turnover|team1_won|sample weight||---|---|---|---||35|15|1|0.1086||28.5|12|0|0.1086||40|18|1|0.1086||25.0|20.0|0|0.1086||37.5|14.0|1|0.1086||30.0|16.0|0|0.1739||42.0|19.5|1|0.1086||33.5|21.0|0|0.1739||team1_3point|team2_turnover|team1_won|sample weight||---|---|---|---||35|15|1|0.1086||28.5|12|0|0.1086||40|18|1|0.1086||25.0|20.0|0|0.1086||37.5|14.0|1|0.1086||30.0|16.0|0|0.1739||42.0|19.5|1|0.1086||33.5|21.0|0|0.1739||team1_3point|team2_turnover||---|---||35|15||28.5|12||40|18||25.0|20.0||37.5|14.0||30.0|16.0||42.0|19.5||33.5|21.0||team1_3point|team2_turnover||---|---||33.5|21.0||28.5|12||30.0|16.0||33.5|21.0||28.5|12||40|18||30.0|16.0||25.0|20.0||team1_3point|team2_turnover|team1_won|sample weight||---|---|---|---||33.5|21.0|0|1/8||28.5|12|0|1/8||30.0|16.0|0|1/8||33.5|21.0|0|1/8||28.5|12|0|1/8||40|18|1|1/8||30.0|16.0|0|1/8||25.0|20.0|0|1/8||team1_3point|team2_turnover|team1_won|sample weight||---|---|---|---||33.5|21.0|0|1/8||28.5|12|0|1/8||30.0|16.0|0|1/8||33.5|21.0|0|1/8||28.5|12|0|1/8||40|18|1|1/8||30.0|16.0|0|1/8||25.0|20.0|0|1/8|**team1_3point** **team2_turnover** **0.24**This is a limited example.You should instead expectto have 10,20, or 30 treeson a dataset **of thousands****if not millions of rows.**|team1_3point|team2_turnover||---|---||21|18||36|17||39|15|## **Boosting - Gradient Boosting**### **Gradient Boosting**## **End of Class Announcements**',\n",
       " 'Shopping Dataset Case Study.pdf': '# **Shopping Dataset Case** **Study**### **Agenda - Goals** Apply basic and intermediate pandas methods to **explore a structured dataset** - Perform **univariate and bivariate analysis** on real-world shopping data Create visualizations using seaborn to support your findings Use grouping and aggregation techniques such as groupby(), pivot_table(), qcut(), andagg() Develop and **communicate insights clearly based on observed data patterns**, not justthe code used## **Shopping Dataset Case Study**You are a Data Analyst for *FlastFash*, a Budapest-based online clothingstore that\\'s looking to break into the American market.### **Shopping Dataset Case Study**Prepare a report for your manager by answering the listed reflectionquestions!## **Visualizing Data - Seaborn**### **Visualizing Data - Bar Graph**Notice that thesex-axis labels aren’teasy to read…#### **df.value_counts(“Color”).plot.bar()**We can quickly plot the frequencies of categories by specifying a**categorical column** in the **value_counts** method, and then by calling the**plot.bar** () method.#### **df.value_counts(“Color”).plot.barh()**### **Visualizing Data - Histogram**What can we do tobetter observe ourdistributions?#### **df[“Purchase Amount (USD)”].plot.hist()**By specifying a **numerical column** and then calling the **plot.hist()** method,we can plot a histogram on a numeric series to observe the distribution ofour dataset.#### **df[“Purchase Amount (USD)”].plot.hist(bins=30)**By **increasing the number of bins**, we can better observe the distributionsthat are apparent in our dataset. What kind of distribution do we see here?#### **sns.scatterplot(df, x=\"Age\", y=\"Purchase Amount (USD)\")**By specifying a **dataframe, and two numerical columns** in the **scatterplot**method, we can plot a scatter-plot to observe the relationship between twonumeric variables. **Do you notice any correlation between age and****purchase amount?**#### **corr** **df[[\"Age\", \"Purchase Amount (USD)\"]]. ()**As we see from the correlation matrix, there is no correlation between ageand purchase amount. However can you identify **clusters** of data whichmight be emerging between these two variables?### **Visualizing Data - Box Plot**#### **sns.boxplot(df, x=\"Review Rating\", y=\"Purchase Amount (USD)\")**By specifying a **dataframe, one categorical, and one numerical column** inthe **boxplot** method, we can plot a box-plot to observe how a distributionvaries across categories. **Do you notice any sizeable differences in median?**## **Shopping Dataset Case Study**Complete this analysis and meet back at 9:20 to answer analytical questionsvia the wheel.## **TLAB #3****Doing your Own EDA**',\n",
       " 'SQL Review II.pdf': '# **SQL Review II**### **Agenda - Announcements**### **Agenda - Goals****●****Simulate the interview process by asking you to complete and present****technical challenges live.****●****Continue working on TLAB #3**## **LeetCode Q1**## **LeetCode Q2**## **LeetCode Q3**## **LeetCode Q4**## **TLAB #3**',\n",
       " 'SQL Review.pdf': '# **SQL Review**### **Agenda - Announcements**### **Agenda - Goals****●****Simulate the interview process by asking you to complete and present****technical challenges live.****●****Continue working on TLAB #3**## **LeetCode Q1**## **LeetCode Q2**## **LeetCode Q3**## **LeetCode Q4**## **TLAB #3**',\n",
       " 'Transformer Architecture.pdf': \"# **A Quick Run-Through of** **Transformer** **Architecture**### **Agenda - Goals**#### **●** **Understand how LLMs work at a high level** **●** **Big idea behind Attention** **●** **Why Attention sparked the modern AI Boom**## **Text Prediction via Neural** **Networks**In more niche circles, we could even apply the transformer architecture to EEG wavesto text (aka mind-reading).## **LLMs and Attention**### **LLMs**For now we want you to think of LLMs as an advancedversion of autocomplete, given a prompt.Rather than providing options of words to complete a phrase,an LLMs output contains a probability distribution of thenext likely word.Instead of always selecting the most probable word however,we can adjust a parameter called **temperature**, such that wesee a bit of randomness in the next outputted word.The recognition of these traits were **learned independently by the neural network itself.**Similarly as the word embedded vectors pass through the transformer architecture, eachhead of attention learns different patterns within the body of text#### Innovations in hardware allow us to run these neural networks. Attention models can process all words at once. Without parallel hardware, LLMs would not work!Almost all of the technology behind this third AI boom has come from the transformerarchitecture! Although originally invented for text, (machine translation) it can be used onimage data, audio data, biological data (protein folding) and much more!In short, transformers aren't just good at reading, they're good at any problem where themodel needs to look at a lot of data and decide what's important.\",\n",
       " 'Twitter Dataset Case Study.pptx.pdf': '# **Twitter Dataset Case** **Study**#### **Agenda - Goals** Perform time series exploration using pandas Extract key date-time features like hour and weekday Analyze and visualize patterns in tweet volume and sentiment Use string matching techniques to filter tweets by content Focus on developing insightful conclusions, not just writing code## **Twitter Dataset Case Study**You are a Data Scientist for *SuperEgo* an NYC-based research institutelooking to create a language model that closely emulates a twitter-user.#### **Twitter Dataset Case Study**Like yesterday, we will use the first-half of class to work on this case studytogether.After break, we will ask you to complete this case study in your groups.We will congregate back at 9:20 to discuss results (with the wheels help).## **Pandas Time Series Data**#### **Pandas Review - Time Bound Data****NOTE:** If it takes more than 2minutes to make your plot (and itcomes out looking like nonsense),you’ve made the **wrong plot.****Don’t make plots just to make plots.**##### **sns.lineplot(df, x=\"date\", y=\"sentiment\")****df[“Date”] = pd.to_datetime(df[“Date”])**Before we do ANYTHING! We must ensurethat our date column is being treated as a“datetime” column.We can do this by calling the “ **to_datetime** ()”method on our date column, and reassigning itback into our original column. Remember, if wedon’t save our changes, we lose them!|ID|Date|Amount||---|---|---||1|Mon Apr 17 20:30 2023|5.24||2|Mon Apr 17 20:31 2023|10.98||3|Mon Apr 17 20:35 2023|1.58||4|Mon Apr 17 23:35 2023|50.60||5|Tues Apr 18 04:20<br>2023|34.01||6|Tues Apr 18 07:50<br>2023|25.05||7|Thurs Apr 20 12:50<br>2023|5.63|For demonstration purposes, let’s look at a dataset of **transactions** in an online shoppingplatform.This dataset is not in our case study, but it will help us understand the resample method.**df.resample(“1D”, on=”Date”)**|ID|Date|Amount||---|---|---||1|Mon Apr 17 20:30 2023|5.24||2|Mon Apr 17 20:31 2023|10.98||3|Mon Apr 17 20:35 2023|1.58||4|Mon Apr 17 23:35 2023|50.60||5|Tues Apr 18 04:20<br>2023|34.01||6|Tues Apr 18 07:50<br>2023|25.05||7|Thurs Apr 20 12:50<br>2023|5.63|**df.resample(“1D”, on=”Date”)**|ID|Date|Amount||---|---|---||1|Mon Apr 17 20:30 2023|5.24||2|Mon Apr 17 20:31 2023|10.98||3|Mon Apr 17 20:35 2023|1.58||4|Mon Apr 17 23:35 2023|50.60||5|Tues Apr 18 04:20<br>2023|34.01||6|Tues Apr 18 07:50<br>2023|25.05||7|Thurs Apr 20 12:50<br>2023|5.63|**df.resample(“1D”, on=”Date”)[“Amount”].mean()**Note that we also getNaN values. These aredays that **don’t have****data**|ID|Date|Amount||---|---|---||1|Mon Apr 17 20:30 2023|5.24||2|Mon Apr 17 20:31 2023|10.98||3|Mon Apr 17 20:35 2023|1.58||4|Mon Apr 17 23:35 2023|50.60||5|Tues Apr 18 04:20<br>2023|34.01||6|Tues Apr 18 07:50<br>2023|25.05||7|Thurs Apr 20 12:50<br>2023|5.63|Just like with **groupby**, we can add a method to calculate some **summary****statistic** . This will select all rows that fall on **the same day…****df.resample(“1D”, on=”Date”)[“Amount”].mean()**Note that we also getNaN values. These aredays that **don’t have****data**|Date|Amount||---|---||Mon Apr 17 2023|17.1||Tues Apr 18 2023|29.53||Wednesday Apr 19 2023|NaN||Thurs Apr 20 2023|5.63|##### **df[\"date\"] = pd.to_datetime(df[\"date\"])** **df.resample(on=\"date\", rule=\"1D\")[\"sentiment\"].mean()**Let’s apply these same principles to our Twitter dataset to better visualizeour former line plot. First we will convert our “date” column to a date-timetype.##### **df[\"date\"] = pd.to_datetime(df[\"date\"])** **df.resample(on=\"date\", rule=\"1D\")[\"sentiment\"].mean()**Now we can resample our dataframe to calculate the average sentimentspers day. **While this is a good start**, how can we plot these values using aline plot?##### **df[\"date\"] = pd.to_datetime(df[\"date\"])** **df.resample(on=\"date\", rule=\"1D\")[\"sentiment\"].mean().plot.line()**###### **df.resample(on=\"date\", rule=\"1D\")[\"sentiment\"].mean().interpolate().plot.line()**## **Regex**As this is a Twitter dataset, we should probably utilize some sort oftext-based analysis. What have we learned about in the past which will helpus figure out what people are talking about?#### **Regular Expressions (Regex)****review**_Farukh is great__Where is Farukh?__Farrrrukh is terrible__I like Python27__@@@19586_|Regex Pattern|Col2||---|---||||_Farukh_**review**_Farukh is great__Where is Farukh?__Farrrrukh is terrible__I like Python27__@@@19586_For example, by just using the regex string “Farukh”, we will look for all rowsthat contain the string “Farukh.” Which rows will be matched?|Regex Pattern|Col2||---|---||||_Farukh_|Regex Pattern|Col2||---|---||||Remember, acomputer does notunderstand intent. Itwill only do exactlywhat you want it to do|Regex Pattern|Col2||---|---||||_Farukh_**review**_Farukh is great__Where is Farukh?__Farrrrukh is terrible__I like Python27__@@@19586_|Regex Pattern<br>^Far*ukh|review||---|---||Regex Pattern<br>_^Far*ukh_|_Farukh is great_||Regex Pattern<br>_^Far*ukh_|_Where is Farukh?_||Regex Pattern<br>_^Far*ukh_|_Farrrrukh is terrible_||Regex Pattern<br>_^Far*ukh_|_I like Python27_||Regex Pattern<br>_^Far*ukh_|_@@@19586_|By placing a caret at the front, we only find reviews that that begin with theword “Farukh” with an arbitrary number of r’s/|Regex Pattern|Col2||---|---|||||Regex Pattern|Col2||---|---||||Knowing regex will save you **hours of work** .#### **Regular Expressions (Regex)**Find all participants wholive in states startingwith “New”|age|age_spouse|State|income||---|---|---|---||32|35|New York|30,000||48|47|New Mexico|70,000||21|NA|California|20,0000|Find all participants wholive in states startingwith “New”|age|age_spouse|State|income||---|---|---|---||32|35|New York|30,000||48|47|New Mexico|70,000||21|NA|California|20,0000|Find all participants wholive in states startingwith “New”### **df[df.State.str.contains(“^New”)]**|age|age_spouse|State|income||---|---|---|---||32|35|New York|30,000||48|47|New Mexico|70,000||21|NA|California|20,0000||user|tweet||---|---||Daniiej|omg i\\'ve an economics test.||sensuoushel<br>p|FOX and the contestants won\\'t go about it<br>right||Taj_Milahi|good luck bro on the test||user|tweet||---|---||Daniiej|omg i\\'ve an economics test.||sensuoushel<br>p|FOX and the contestants won\\'t go about it<br>right||Taj_Milahi|good luck bro on the test||user|tweet||---|---||Daniiej|omg i\\'ve an economics test.||sensuoushel<br>p|FOX and the contestants won\\'t go about it<br>right||Taj_Milahi|good luck bro on the test|## **Twitter Dataset Case Study**Complete this analysis and meet back at 9:20 to answer analytical questionsvia the wheel.## **TLAB #3**',\n",
       " 'Types of Visualizations Review.pdf': '# **Types of Visualizations** **Review**|“The existence of Comet NEOWISE (here depicted as a series|Col2||---|---||_of red dots) was discovered by analyzing astronomical survey_|_of red dots) was discovered by analyzing astronomical survey_||_data…”_||### **Agenda - Goals****●****Understand which visualizations to choose for your analysis****●****Learn how to make data visualizations in Python****●****Understand the fundamentals of matplotlib**## **Warm-Up**## **Review of Variables**### **Visualizing Data**### **Independent vs Dependent Variable**### **Quantitative vs Categorical Variables**Now that we’re equipped with these terms, which **variable is quantitative**and which **variable is categorical** ?Now that we’re equipped with these terms, which **variable is quantitative**and which **variable is categorical** ?Again, let’s continue to define these terms. Is our dependent variable**continuous** or **discrete** ?## **Visualizing Data****Univariate** **Bivariate** **Multivariate**### **Visualizing Data - Describing vs Prescribing**### **Visualizing Data - Descriptive Analytics**In descriptive analytics, we are:_**●**__**Exploring what occurred in the past**__**●**__**Identifying anomalies**__**●**__**Identifying relationships and patterns**_**Ex** :_Current users using our platform,__last-years sales, historical placement rates,__current flying conditions (wind, temp, etc)_### **Visualizing Data - Bar Graph**### **Visualizing Data - Histogram**### **Visualizing Data - Box Plot**### **Visualizing Data - More Visualizations**## **Creating Visualizations in** **Python**### **Matplotlib - General Form****import matplotlib.pyplot as plt****fig, ax = plt.subplots()****ax.plot([1,2,3,4,3,5,2,4])****ax.set_title(“Test Plot”)****ax.set_xlabel(“x-axis”)****ax.set_ylabel(“y-axis”)****fig.savefig(“test.png”)**Since matplotlib is not a built in package, you must first download it via pipand then import it to all subsequent code**ax.pie() # create pie chart****ax.bar() # create bar chart****ax.scatter() # create scatter plot****ax.set_ylim() # set y-axis range****fig.savefig() # save image****fig.show() # show image****fig.clear() # clear image**With this review of methods, let’s break down what these **axes** methods do.As well as what our **figure** method does. This creates a new image!Notice these are _almost_ the same as the implicit methods, except this timewe only use the **plt** package name to call of our methods.We can use the **bar()** plot to make bar graphs (2 lists, just like with ourline-plot)Often times when you do data analysis, you need to increase the number ofbins to capture capture the distribution of your dataset.Notice that by increasing the number of bins, the more “detail” we can see.At some point however, we get “diminishing” returns in understandability,**so don’t go too far here.****More polygons → More resolution**## **Common Matplotlib Problem**### **Common Matplotlib Problems**## **Wrap-Up**## **Glossary****population** - entire group you could possibly get data from**sample** - a subset of your population which you collect data from**feature** - an important component of your data**independent variable** - something that does not change when other features in your data change**dependent variable** - a value which changes when other features in your data change**discrete variable** - a variable which is measured in whole numbers (think integers)**continuous variable** - a variable which is measured with all real numbers, including decimals**categorical variable** - something that is describes things (e.g. color, type, class, etc;)**quantitative variable** - something that measures things (e.g. age, height, weight, etc;)**latent variable -** a variable that is not directly observable, but can be inferred from other variables that can bedirectly measured'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_dict = {key: value.replace(\"\\n\", \"\").replace(\"\\r\", \"\") for key, value in slides.items()}\n",
    "cleaned_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9d5cf180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# **Introduction to** **K-Nearest-Neighbors**#### **Agenda - Goals**##### **●**## **Warm Up**## **k-Nearest Neighbors**#### **k-Nearest Neighbors (kNN)**#### **k-Nearest Neighbors (kNN)**##### You recognize some cybersecurity fellows, as well as data science fellows. **You notice that everyone is congregating and chatting in groups.**##### But then, you notice someone that you do not recognize. Considering however that all cyber-security fellows are sticking together, which **fellowship does this person most likely belong to?**##### Most likely a cyber-security fellow, no?##### What if we have someone that’s on the periphery of both groups ? We’ll solve this during our discussion of the kNN algorithm.#### **k-Nearest Neighbors (kNN)**We formalize this idea via the following formula for conditional probability. Once again let’s break thisdown before moving to an example.The likelihood the **class of this sample is “j”,** given **the point x** **0** is equal to*Note, we identify the set of “K” nearest points as N 0 . There’snothing special about this name (for now), we could have named it“Roofus” if we wanted to.The likelihood the **class of this sample is “j”,** given **the point x** **0** is equal toThe likelihood the **class of this sample is “j”,** given **the point x** **0** is equal toThe likelihood the **class of this sample is “j”,** given **the point x** **0** is equal toYou may be thinking “ _Darn that’s a whole lot of symbols to say something so simple_ .”However, when it comes to creating ideas in maths, we **cannot be ambiguous** . Therefore weformalize these ideas in a **common set of symbols that all humans agree on** . It’s a beautifulthing really.Succinctly put, we estimate the **conditional probability that x0 belongs to class j as the fraction of the K****nearest points** **whose response values equal j** .Let’s bring back our kibble/loud dataset that identifies **cats, hamsters, and dogs** . I reduced thedata-points to make this graph easier to discuss. You may notice the axes now have equal range, this willI introduce a mystery animal. Let’s assume they are located on coordinate (40, 30). Let’s estimate theprobability that this animal is a cat, dog, or hamster using our **estimated conditional probability.**#### **Small Aside - Euclidean Distance (L2)**#### **Small Aside - Euclidean Distance (L2)**#### **Small Aside - Euclidean Distance (L2)**#### **Small Aside - Euclidean Distance (L2)**#### **Small Aside - Euclidean Distance (L2)**#### **Small Aside - Euclidean Distance (L2)**#### **Small Aside - Euclidean Distance (L2)**#### **Small Aside - Manhattan Distance (L1)**Now, picture we can only travel in straight lines andturn in right angles (as if we were on the streets of**manhattan** ).This entails simply adding the sides of our righttriangle!Can anyone calculate this?#### **Small Aside - Manhattan Distance (L1)**Now, picture we can only travel in straight lines andturn in right angles (as if we were on the streets of**manhattan** ).This entails simply adding the sides of our righttriangle!Can anyone calculate this?And perhaps propose a formula for us to use?#### **Small Aside - Manhattan Distance (L1)**##### ⅔ ⅓We’re going to pause our discussion ofkNN here.However there is more to evaluate whendiscussing this **simple, yet powerful****algorithm.**What are some questions you mighthave regarding kNN?We’re going to pause our discussion ofkNN here.However there is more to evaluate whendiscussing this **simple, yet powerful****algorithm.**What are some questions you mighthave regarding kNN?**●****Why are my axes normalized?****●** **When do we use euclidean vs****manhattan?****●****What happens if we have****categorical data?****●****What happens as I increase my****dimensions?****●** **What are some cons to kNN?**##### Going back to our TKH industry event discussion, we simply select some “K” that accurately classifies this person as a data or cyber fellow.##### **k=3**##### **Class = Cyber**##### **k=5**##### **Class = Cyber**##### Class = Cyber ( are we suffering from imbalance??? )## **Choosing a K - HyperParam**##### Let’s bring back our scatter plot (with a twist!) To obtain the decision boundary for kNN, we could compute the class using kNN for eachRed = cat; yellow = hammie; blue = dog##### We begin with k=1. What do you notice about the error rate? Does it make any mistakes?Red = cat; yellow = hammie; blue = dog##### None whatsoever! While our “training” rate might seem ideal, let’s see what happens when we introduce a new test sample. Who is our hamster friend **classified as?**##### A cat . Remember everyone, a classifier that perfectly fits our training dataRed = cat; yellow = hammie; blue = dog##### We move onto k=3. What happens to our training error rate now?Red = cat; yellow = hammie; blue = dog##### Unfortunately a few training samples are classified incorrectly, but this could actually be preferable when it comes to our testing data !##### Bringing back our hamster, we see that it is now correctly classified as a hamster (ignore the roughness of my sketches)Red = cat; yellow = hammie; blue = dog##### Finally lets try k=6. What happens to our training error rate now?#### **kNN - Importance of Data Scaling**##### _X1: (-2 to 2)_ _X2: (-2 to 2)_ Now that we’ve standardized our dataset, we can assign equal **importance to dimensions.** Subsequently, we update our predictions.#### **kNN**###### To conclude our conversation on the supervised learning classifier kNN, it is a **powerful non-parametric supervised learning algorithm that utilizes a** . **fairly “lazy” classification method** Pros ● No optimization involved ● Comparable performance to Naive Bayes **●** **Easy to understand** Cons ● Sensitive to class imbalance ● Need to store entire training dataset for prediction                       Good visual of kNN from https://knn [notebook.netlify.app/](https://knn-notebook.netlify.app/)##### Let’s review the following gif to see the process that kNN takes to classify a new test observation.## **Classification Challenges**#### **Classification Challenges**#### **Choosing a Distance Metric**#### **Choosing a Distance Metric**What do you notice about the **sparsity** of our data as we increase dimensions?Notice how the frequency of distances converges on a specific value as we increase dimensions:[https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote02_kNN.html](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote02_kNN.html)#### **The Curse of Dimensionality**##### Lower dimensional manifold in a nutshell: “ Wow this looks really complex… ”#### **Handling Data Imbalance**_True Positives + True Negatives__False Negatives_##### This leads to what we call the **accuracy** paradox (null accuracy). Let’s say you go to another industry event and you see 9 cyber fellows and 1 data fellow. You train a kNN algorithm and it gives you a 90% accuracy rate. Wow! This must be a **good model, no?**_True Positives + True Negatives__False Negatives__9 + 0_**90%**_9 + 0_**90%**#### **Over-Sampling Vs Under-Sampling**#### **Over-Sampling Vs Under-Sampling**#### **SMOTE**_Synthetic Minority Over-sampling Technique_This technique entails creating new _synthetic minority classes_ via a somewhat randomized process.1. Choose a random set of the minority class2. Find k-nearest-neighbors for each sample of the same class3. Calculate a new synthetic class between this sample and all of its neighborsa. Choose a random value between 0 & 1 to calculate where this newly random point willexist in the space between your data point and its neighbors##### This allows us to “purposefully” oversample our dataset to improve the predictive qualities of our dataset.#### **Handling Data Imbalance**##### This is especially useful when trying to detect some real relatively rare class in your dataset: ● Actual fraud **● A rare disease** ● **A 16-seeded team beating a 1-seeded team**## **End of Class Announcements**##### ○ Where does variance/bias exist in kNN?|Neural network training|makes||---|---||**_beautiful fractals_**||',\n",
       " '# **Natural Language** **Processing & Vector** **Embeddings**### **Agenda - Goals****●** **…**## **Kahoot**Let’s begin todays Kahoot## **Pre-Class Review**### **Pre-Class Review**## **Natural Language Processing**### **Natural Language Processing**### **Natural Language Processing Techniques**As it turns out, computer scientists have been attempting to solve this problemfor a quite a long time. Let’s review the foundational techniques of analyzing a“corpus” of text._**●**__**Bag of Words**_ (Harris 1954)_**●**_ _**TF-IDF**_ (Jones 1972)These techniques are dated by this point, **but it doesn’t mean these techniques****are useless.** Let’s see what their purpose is in the world of document analysis.### **Bag of Words**Let’s take three example text files and see how the BOW technique analyzes them.We will use **3 example reviews from Amazon and their subsequent reviews (out****of 5 stars)** .What’s the first step to the BOW technique?**(1)****(2)**### **Term Frequency-Inverse Document Frequency** **(TF-IDF)**While BOW is a trivial technique to interpret text, we can iterate upon thisimplementation via the **Term Frequency-Inverse Document Frequency** algorithm.The steps to this include:_**1.**_ _**BOW steps up to step 2**__**2.**_ _**Calculate the total number of times word t appears in document d, divided by all the**__**terms in document d (term frequency)**__**3.**_ _**Calculate the total number of documents divided by the number of documents**__**containing term t (inverse document frequency)**__**4.**_ _**Multiply these two values together to calculate the tf-idf.**_## **Vector Embeddings**### **Word Embeddings - Data Representations**### **Word Embeddings - Word2Vec**By observing wheres these data-samples exist in the **semantic feature**Lastly, to demonstrate the power of word2vec transformations, let’s say wehave the additional data-samples “ **evening** ” and “ **dinner** .”Try adding the vectors “lunch” + “evening” [1, 4] + [3, -3].Which value do we get?### **Word Embeddings - Word2Vec**### **Word Embeddings - Word2Vec**### **Word Embeddings - Word2Vec (SkipGram)**Let’s observe the steps to train a **skip-gram word2vec**_**1.**_ _**Tokenize your text into words (or n-grams)**__**2.**_ _**Eliminate stop-words and punctuation**_**3.** **Choose a window size of “n”****4.** **Loop through each window of your corpus****a.****Train your single hidden layer neural network on each (n) context****words & target word**_**The dog eats the bone**__**The cat eats fish.**__**The cat scratches its owner.**_## **Embedding Everything**### **Transformer Embeddings**## **End of Class Announcements**',\n",
       " '# **Probability Review**#### **Agenda - Schedule**#### **Agenda - Announcements**#### **Agenda - Goals****●****Review basic probability****●****Understand how sample calculations differ from population****●****Understand & apply Bayes Theorem**## **Inferring from a Sample**We often use humans when discussing population vs sample, but thisapplies **to any dataset** . Ex: All AAPL stock prices since Dec 14, 1984( **population** ) & AAPL stock price on Dec 12, 2024 ( **sample** ).Our equations for measures of dispersion change based on which subset weare calculating our metrics on. This is **because a sample is an estimate**, and a.**population is ground truth**## **Probability Review**|Event|Probability||---|---||**Dog**|0.5||**Cat**|0.3||**Hamster**|0.1||**Lizard**|0.0||**No pet**|0.1|Let’s increase our sample space in order to discuss more complex terms.Here we describe the probabilities that an american household has somespecific pet.**Dog** **Cat** **Hamster** **Lizard** **None**Graphing these values gives us the **probability distribution** .|Event|Probability||---|---||**Dog**|0.5||**Cat**|0.3||**Hamster**|0.1||**Lizard**|0.0||**No pet**|0.1|## **Probability in Data Analysis -** **Bayes Theorem**#### **Probability in Data Analysis**#### **Bayes Theorem - Frequentist World-View**#### **Bayes Theorem - Frequentist Pros**#### **Bayes Theorem - Frequentist Cons**#### **Bayes Theorem - Frequentist Cons**#### **Bayes Theorem - Bayesian Statistics**_statistician_## **P(H) P(E|H)**Here’s the formula, let’s **break down its components** before going throughour covid example.Remember, the “|” is the symbolfor conditional probabilitystatements.“Hypothesis given Event”## **P(H) P(E|H)**The probability the **hypothesis** is true given the **event** is equal toThe probability the **hypothesis** is true given the **event** is equal toThe probability the **hypothesis** is true given the **event** is equal to## **P(H) P(E|H)**Divided by the probability of **event** occurring. This part is sometimesdeceptively simple. Keep in mind that we want to consider the **event occurs****given the hypothesis is true and given the hypothesis is false** !Let’s understand how to calculate this. I find diagrams to be the most helpfulin visualizing this calculationTo calculate P(E), we have to consider the intersection of P(E) and P(H), aswell as the space where P(E) exists outside of P(H)This intersect can be labeled **P(E & H)**In terms of conditional probability, that is **P(H)P(E|H)**And lastly we have **P(E & -H).** Can anyone express this in terms ofconditional probability as well?**P(-H)P(E|-H). This allows us to express P(E) as P(H)P(E|H) + P(-H)P(E|-H)**## **P(H) P(E|H)****P(H|E) = 0.089****P(H|E) = 0.47**## **Wrap-Up**',\n",
       " '# **Random Forests &** **Boosted Forests**### **Agenda - Schedule**### **Agenda - Goals****●**## **Bagging**|team1_3point|team2_turnover|team1_won||---|---|---||35|15|1||28.5|12|0||40|18|1||25.0|20.0|0|1 0 1 0And this is only for our4-sample dataset. Whatif we have 100, 1000 or1,000,000 samples?|team1_3point|team2_turnover|team1_won||---|---|---||35|15|1||28.5|12|0||40|18|1||25.0|20.0|0|Going back to the formulation of our decision trees, our trees will **overfit to****this noisy data and fail to generalize on new test samples.**Is this considered high bias or high variance?1 0 1 0|team1_3point|team2_turnover|team1_won||---|---|---||35|15|1||28.5|12|0||40|18|1||25.0|20.0|0|Therefore, we need a method to cut through all this noise and somehow**extract the true signal from this dataset** . The first technique we will learn is.**bagging**### **Bagging - Bootstrapping**### **Bagging - Bootstrapping**### **Bagging - Bootstrapping + Aggregating**Let’s apply this technique to our decision trees inorder to implement what we call “ **bagged trees.** ”The algorithm for **bagging (bootstrapping +****aggregating)** is applied through the following steps:1. Select _n_ random subsets of datasets withreplacement ( **bootstrap** )2. Grow arbitrarily large trees on each subset3. “ _Democratically_ ” select the prediction whichhas the most votes across all trees( **aggregate** ).|team1_3point|team2_turnover|team1_won||---|---|---||35|15|1||28.5|12|0||40|18|1||25.0|20.0|0||37.5|14.0|1||30.0|16.0|0||42.0|19.5|1||33.5|21.0|0|Let’s bring back our basketball dataset to see how we can apply **bagging** andstrengthen the true signal of our dataset. We include a few more data pointsto make this a little more interesting.|28.5|12|0||---|---|---||37.5|14.0|1||30.0|16.0|0||Col1|35|15|1||---|---|---|---|||42.0|19.5|1|||28.5|12|0||40|18|1||---|---|---||33.5|21.0|0||25.0|20.0|0||team1_3point|team2_turnover|team1_won||---|---|---||35|15|1||28.5|12|0||40|18|1||25.0|20.0|0||37.5|14.0|1||30.0|16.0|0||42.0|19.5|1||33.5|21.0|0||Col1|28.5|12|0||---|---|---|---|||42.0|19.5|1|||28.5|12|0|First, we will make 4 random subsets of data (with replacement). This is the**bootstrap** step.**Note** : keep in mind thatthis example is toosimple to make anymeaningful tree|28.5|12|0||---|---|---||37.5|14.0|1||30.0|16.0|0|1 0|35|15|1||---|---|---||42.0|19.5|1||28.5|12|0||40|18|1||---|---|---||33.5|21.0|0||25.0|20.0|0||28.5|12|0||---|---|---||42.0|19.5|1||28.5|12|0|1 0|team1_3point|team2_turnover|team1_won_pred||---|---|---||35|15|???||28.5|12|???||40|18|???||25.0|20.0|???||37.5|14.0|???||30.0|16.0|???||42.0|19.5|???||33.5|21.0|???|1 0|team1_3point|team2_turnover|team1_won_pred||---|---|---||35|15|???||28.5|12|???||40|18|???||25.0|20.0|???||37.5|14.0|???||30.0|16.0|???||42.0|19.5|???||33.5|21.0|???|1 010|team1_3point|team2_turnover|team1_won_pred||---|---|---||35|15|???||28.5|12|???||40|18|???||25.0|20.0|???||37.5|14.0|???||30.0|16.0|???||42.0|19.5|???||33.5|21.0|???|One question that youmight be thinking is“wait so now we’veintroduced a parameter’to indicate how manytrees we will train, isthis anotherhyperparameter thatwe must find viaGridSearch?”1 0101 0|team1_3point|team2_turnover|team1_won_pred||---|---|---||35|15|**1**||28.5|12|???||40|18|???||25.0|20.0|???||37.5|14.0|???||30.0|16.0|???||42.0|19.5|???||33.5|21.0|???|1 01 0tie|team1_3point|team2_turnover|team1_won_pred|team1_won||---|---|---|---||35|15|**1**|1||28.5|12|**0**|0||40|18|**1**|1||25.0|20.0|**0**|0||37.5|14.0|**1**|1||30.0|16.0|**0**|0||42.0|19.5|**1**|1||33.5|21.0|**0 or 1**|0|Just for fun, let’s fill out the rest of these samples and observe the accuracy. Noticethat we occasionally get **ties.** In this case, we decide randomly which class tochoose which leads us to an accuracy that ranges from **85% to 100%**As it turns out, the number of trees that we train _**“B”**_ does not suffer from overfitting (high variance) as weincrease this amount! Instead, error simply _settles_ after enough trees are generated. **WOW!**Therefore, we simplyuse a **sufficiently large****B** until we do not seelarge gains in testaccuracy.One technique we canuse is to start large(100), and then see howmany trees we can**remove for the same****test accuracy rate.**Impurity Removed = 0.8751 0Impurity Removed = 0.33Impurity Removed = 11 0Impurity Removed = 0.8751 0By adding all the impurity removed, we can observe which predictor is the **most important** . In thisdataset, which feature seems to remove the most impurity?## **Random Forests**### **Random Forests**|team1_3point|team2_turnover|team1_won||---|---|---||35|15|1||28.5|12|0||40|18|1||25.0|20.0|0||37.5|14.0|1||30.0|16.0|0||42.0|19.5|1||33.5|21.0|0|Let’s go back to our basketball example once more, and walk through thesteps of bagging with this additional caveat|28.5|12|0||---|---|---||37.5|14.0|1||30.0|16.0|0||Col1|35|15|1||---|---|---|---|||42.0|19.5|1|||28.5|12|0||40|18|1||---|---|---||33.5|21.0|0||25.0|20.0|0||team1_3point|team2_turnover|team1_won||---|---|---||35|15|1||28.5|12|0||40|18|1||25.0|20.0|0||37.5|14.0|1||30.0|16.0|0||42.0|19.5|1||33.5|21.0|0||Col1|28.5|12|0||---|---|---|---|||42.0|19.5|1|||28.5|12|0|First, we will make 4 random subsets of data (with replacement). This is the**bootstrap** step.|28.5|12|0||---|---|---||37.5|14.0|1||30.0|16.0|0||35|15|1||---|---|---||42.0|19.5|1||28.5|12|0||40|18|1||---|---|---||33.5|21.0|0||25.0|20.0|0||28.5|12|0||---|---|---||42.0|19.5|1||28.5|12|0|1 01 0|team1_3point|team2_turnover|team1_won_pred|team1_won||---|---|---|---||35|15|**1**|1||28.5|12|**0**|0||40|18|**1**|1||25.0|20.0|**0**|0||37.5|14.0|**1**|1||30.0|16.0|**0**|0||42.0|19.5|**1**|1||33.5|21.0|**1**|0|Once again, we allow these trees to democratically elect the prediction based on their **bootstrapped****samples and limited predictors. Notice that we get no ties! We walk away with one accuracy!**This outcome is not unique to our dataset. Notice the **Test:RandomForest** error rate performs noticeably**better than the simple bagged model** .- Computationally **expensive**## **Boosting - AdaBoost****Trees with better information on****which mistakes they should avoid****should also be better classifiers!****Metaphors get tough in this context****but consider the following:****You, a New Yorker/Connecticuter,****are voting for public transportation****policies in your city. Might you have****better context than an extra-state****voter?**|28.5|12|0||---|---|---||37.5|14.0|1||30.0|16.0|0|1 0|35|15|1||---|---|---||42.0|19.5|1||28.5|12|0||40|18|1||---|---|---||33.5|21.0|0||25.0|20.0|0||28.5|12|0||---|---|---||42.0|19.5|1||28.5|12|0|### **Boosting - AdaBoost**|team1_3point|team2_turnover|team1_won|sample weight||---|---|---|---||35|15|1|1/8||28.5|12|0|1/8||40|18|1|1/8||25.0|20.0|0|1/8||37.5|14.0|1|1/8||30.0|16.0|0|1/8||42.0|19.5|1|1/8||33.5|21.0|0|1/8|1 0|team1_3point|team2_turnover|team1_won|sample weight||---|---|---|---||35|15|1|1/8||28.5|12|0|1/8||40|18|1|1/8||25.0|20.0|0|1/8||37.5|14.0|1|1/8||30.0|16.0|0|1/8||42.0|19.5|1|1/8||33.5|21.0|0|1/8|Just like with random forests, we will grow **stumps** on a subset of predictors(one tree per predictor). However this time we use the **entire dataset.**= 0= 2= 4= 2Gini = 1 ((0/2)^2+(2/2)^2)= 2= 2= 2= 2|team1_3point|team2_turnover|team1_won|sample weight||---|---|---|---||35|15|1|1/8||28.5|12|0|1/8||40|18|1|1/8||25.0|20.0|0|1/8||37.5|14.0|1|1/8||30.0|16.0|0|1/8||42.0|19.5|1|1/8||33.5|21.0|0|1/8|= 0= 2= 4= 2Gini = 0.44Gini = 0= 2= 2= 2= 2|team1_3point|team2_turnover|team1_won|sample weight||---|---|---|---||35|15|1|1/8||28.5|12|0|1/8||40|18|1|1/8||25.0|20.0|0|1/8||37.5|14.0|1|1/8||30.0|16.0|0|1/8||42.0|19.5|1|1/8||33.5|21.0|0|1/8|= 0= 2= 4= 2Total Gini = 0.44 *(6/8) + 0 * (2/8)= 2= 2= 2= 2|team1_3point|team2_turnover|team1_won|sample weight||---|---|---|---||35|15|1|1/8||28.5|12|0|1/8||40|18|1|1/8||25.0|20.0|0|1/8||37.5|14.0|1|1/8||30.0|16.0|0|1/8||42.0|19.5|1|1/8||33.5|21.0|0|1/8|= 0= 2= 4= 2= 2= 2= 2= 2|team1_3point|team2_turnover|team1_won|sample weight||---|---|---|---||35|15|1|1/8||28.5|12|0|1/8||40|18|1|1/8||25.0|20.0|0|1/8||37.5|14.0|1|1/8||30.0|16.0|0|1/8||42.0|19.5|1|1/8||33.5|21.0|0|1/8|= 0= 2= 4= 2Total Error = ⅛ + ⅛ = **0.25**0 -> Perfect stump1 -> Bad stump|team1_3point|team2_turnover|team1_won|sample weight||---|---|---|---||35|15|1|1/8||28.5|12|0|1/8||40|18|1|1/8||25.0|20.0|0|1/8||37.5|14.0|1|1/8||30.0|16.0|0|1/8||42.0|19.5|1|1/8||33.5|21.0|0|1/8|= 0= 2= 4= 2|team1_3point|team2_turnover|team1_won|sample weight||---|---|---|---||35|15|1|1/8||28.5|12|0|1/8||40|18|1|1/8||25.0|20.0|0|1/8||37.5|14.0|1|1/8||30.0|16.0|0|1/8||42.0|19.5|1|1/8||33.5|21.0|0|1/8||team1_3point|team2_turnover|team1_won|sample weight||---|---|---|---||35|15|1|1/8||28.5|12|0|1/8||40|18|1|1/8||25.0|20.0|0|1/8||37.5|14.0|1|1/8||30.0|16.0|0|1/8||42.0|19.5|1|1/8||33.5|21.0|0|1/8||team1_3point|team2_turnover|team1_won|sample weight||---|---|---|---||35|15|1|1/8||28.5|12|0|1/8||40|18|1|1/8||25.0|20.0|0|1/8||37.5|14.0|1|1/8||30.0|16.0|0|1/8||42.0|19.5|1|1/8||33.5|21.0|0|1/8||team1_3point|team2_turnover|team1_won|sample weight||---|---|---|---||35|15|1|0.10||28.5|12|0|0.10||40|18|1|0.10||25.0|20.0|0|0.10||37.5|14.0|1|0.10||30.0|16.0|0|0.16||42.0|19.5|1|0.10||33.5|21.0|0|0.16||team1_3point|team2_turnover|team1_won|sample weight||---|---|---|---||35|15|1|0.1086||28.5|12|0|0.1086||40|18|1|0.1086||25.0|20.0|0|0.1086||37.5|14.0|1|0.1086||30.0|16.0|0|0.1739||42.0|19.5|1|0.1086||33.5|21.0|0|0.1739||team1_3point|team2_turnover|team1_won|sample weight||---|---|---|---||35|15|1|0.1086||28.5|12|0|0.1086||40|18|1|0.1086||25.0|20.0|0|0.1086||37.5|14.0|1|0.1086||30.0|16.0|0|0.1739||42.0|19.5|1|0.1086||33.5|21.0|0|0.1739||team1_3point|team2_turnover||---|---||35|15||28.5|12||40|18||25.0|20.0||37.5|14.0||30.0|16.0||42.0|19.5||33.5|21.0||team1_3point|team2_turnover||---|---||33.5|21.0||28.5|12||30.0|16.0||33.5|21.0||28.5|12||40|18||30.0|16.0||25.0|20.0||team1_3point|team2_turnover|team1_won|sample weight||---|---|---|---||33.5|21.0|0|1/8||28.5|12|0|1/8||30.0|16.0|0|1/8||33.5|21.0|0|1/8||28.5|12|0|1/8||40|18|1|1/8||30.0|16.0|0|1/8||25.0|20.0|0|1/8||team1_3point|team2_turnover|team1_won|sample weight||---|---|---|---||33.5|21.0|0|1/8||28.5|12|0|1/8||30.0|16.0|0|1/8||33.5|21.0|0|1/8||28.5|12|0|1/8||40|18|1|1/8||30.0|16.0|0|1/8||25.0|20.0|0|1/8|**team1_3point** **team2_turnover** **0.24**This is a limited example.You should instead expectto have 10,20, or 30 treeson a dataset **of thousands****if not millions of rows.**|team1_3point|team2_turnover||---|---||21|18||36|17||39|15|## **Boosting - Gradient Boosting**### **Gradient Boosting**## **End of Class Announcements**',\n",
       " '# **Shopping Dataset Case** **Study**### **Agenda - Goals** Apply basic and intermediate pandas methods to **explore a structured dataset** - Perform **univariate and bivariate analysis** on real-world shopping data Create visualizations using seaborn to support your findings Use grouping and aggregation techniques such as groupby(), pivot_table(), qcut(), andagg() Develop and **communicate insights clearly based on observed data patterns**, not justthe code used## **Shopping Dataset Case Study**You are a Data Analyst for *FlastFash*, a Budapest-based online clothingstore that\\'s looking to break into the American market.### **Shopping Dataset Case Study**Prepare a report for your manager by answering the listed reflectionquestions!## **Visualizing Data - Seaborn**### **Visualizing Data - Bar Graph**Notice that thesex-axis labels aren’teasy to read…#### **df.value_counts(“Color”).plot.bar()**We can quickly plot the frequencies of categories by specifying a**categorical column** in the **value_counts** method, and then by calling the**plot.bar** () method.#### **df.value_counts(“Color”).plot.barh()**### **Visualizing Data - Histogram**What can we do tobetter observe ourdistributions?#### **df[“Purchase Amount (USD)”].plot.hist()**By specifying a **numerical column** and then calling the **plot.hist()** method,we can plot a histogram on a numeric series to observe the distribution ofour dataset.#### **df[“Purchase Amount (USD)”].plot.hist(bins=30)**By **increasing the number of bins**, we can better observe the distributionsthat are apparent in our dataset. What kind of distribution do we see here?#### **sns.scatterplot(df, x=\"Age\", y=\"Purchase Amount (USD)\")**By specifying a **dataframe, and two numerical columns** in the **scatterplot**method, we can plot a scatter-plot to observe the relationship between twonumeric variables. **Do you notice any correlation between age and****purchase amount?**#### **corr** **df[[\"Age\", \"Purchase Amount (USD)\"]]. ()**As we see from the correlation matrix, there is no correlation between ageand purchase amount. However can you identify **clusters** of data whichmight be emerging between these two variables?### **Visualizing Data - Box Plot**#### **sns.boxplot(df, x=\"Review Rating\", y=\"Purchase Amount (USD)\")**By specifying a **dataframe, one categorical, and one numerical column** inthe **boxplot** method, we can plot a box-plot to observe how a distributionvaries across categories. **Do you notice any sizeable differences in median?**## **Shopping Dataset Case Study**Complete this analysis and meet back at 9:20 to answer analytical questionsvia the wheel.## **TLAB #3****Doing your Own EDA**',\n",
       " '# **SQL Review II**### **Agenda - Announcements**### **Agenda - Goals****●****Simulate the interview process by asking you to complete and present****technical challenges live.****●****Continue working on TLAB #3**## **LeetCode Q1**## **LeetCode Q2**## **LeetCode Q3**## **LeetCode Q4**## **TLAB #3**',\n",
       " '# **SQL Review**### **Agenda - Announcements**### **Agenda - Goals****●****Simulate the interview process by asking you to complete and present****technical challenges live.****●****Continue working on TLAB #3**## **LeetCode Q1**## **LeetCode Q2**## **LeetCode Q3**## **LeetCode Q4**## **TLAB #3**',\n",
       " \"# **A Quick Run-Through of** **Transformer** **Architecture**### **Agenda - Goals**#### **●** **Understand how LLMs work at a high level** **●** **Big idea behind Attention** **●** **Why Attention sparked the modern AI Boom**## **Text Prediction via Neural** **Networks**In more niche circles, we could even apply the transformer architecture to EEG wavesto text (aka mind-reading).## **LLMs and Attention**### **LLMs**For now we want you to think of LLMs as an advancedversion of autocomplete, given a prompt.Rather than providing options of words to complete a phrase,an LLMs output contains a probability distribution of thenext likely word.Instead of always selecting the most probable word however,we can adjust a parameter called **temperature**, such that wesee a bit of randomness in the next outputted word.The recognition of these traits were **learned independently by the neural network itself.**Similarly as the word embedded vectors pass through the transformer architecture, eachhead of attention learns different patterns within the body of text#### Innovations in hardware allow us to run these neural networks. Attention models can process all words at once. Without parallel hardware, LLMs would not work!Almost all of the technology behind this third AI boom has come from the transformerarchitecture! Although originally invented for text, (machine translation) it can be used onimage data, audio data, biological data (protein folding) and much more!In short, transformers aren't just good at reading, they're good at any problem where themodel needs to look at a lot of data and decide what's important.\",\n",
       " '# **Twitter Dataset Case** **Study**#### **Agenda - Goals** Perform time series exploration using pandas Extract key date-time features like hour and weekday Analyze and visualize patterns in tweet volume and sentiment Use string matching techniques to filter tweets by content Focus on developing insightful conclusions, not just writing code## **Twitter Dataset Case Study**You are a Data Scientist for *SuperEgo* an NYC-based research institutelooking to create a language model that closely emulates a twitter-user.#### **Twitter Dataset Case Study**Like yesterday, we will use the first-half of class to work on this case studytogether.After break, we will ask you to complete this case study in your groups.We will congregate back at 9:20 to discuss results (with the wheels help).## **Pandas Time Series Data**#### **Pandas Review - Time Bound Data****NOTE:** If it takes more than 2minutes to make your plot (and itcomes out looking like nonsense),you’ve made the **wrong plot.****Don’t make plots just to make plots.**##### **sns.lineplot(df, x=\"date\", y=\"sentiment\")****df[“Date”] = pd.to_datetime(df[“Date”])**Before we do ANYTHING! We must ensurethat our date column is being treated as a“datetime” column.We can do this by calling the “ **to_datetime** ()”method on our date column, and reassigning itback into our original column. Remember, if wedon’t save our changes, we lose them!|ID|Date|Amount||---|---|---||1|Mon Apr 17 20:30 2023|5.24||2|Mon Apr 17 20:31 2023|10.98||3|Mon Apr 17 20:35 2023|1.58||4|Mon Apr 17 23:35 2023|50.60||5|Tues Apr 18 04:20<br>2023|34.01||6|Tues Apr 18 07:50<br>2023|25.05||7|Thurs Apr 20 12:50<br>2023|5.63|For demonstration purposes, let’s look at a dataset of **transactions** in an online shoppingplatform.This dataset is not in our case study, but it will help us understand the resample method.**df.resample(“1D”, on=”Date”)**|ID|Date|Amount||---|---|---||1|Mon Apr 17 20:30 2023|5.24||2|Mon Apr 17 20:31 2023|10.98||3|Mon Apr 17 20:35 2023|1.58||4|Mon Apr 17 23:35 2023|50.60||5|Tues Apr 18 04:20<br>2023|34.01||6|Tues Apr 18 07:50<br>2023|25.05||7|Thurs Apr 20 12:50<br>2023|5.63|**df.resample(“1D”, on=”Date”)**|ID|Date|Amount||---|---|---||1|Mon Apr 17 20:30 2023|5.24||2|Mon Apr 17 20:31 2023|10.98||3|Mon Apr 17 20:35 2023|1.58||4|Mon Apr 17 23:35 2023|50.60||5|Tues Apr 18 04:20<br>2023|34.01||6|Tues Apr 18 07:50<br>2023|25.05||7|Thurs Apr 20 12:50<br>2023|5.63|**df.resample(“1D”, on=”Date”)[“Amount”].mean()**Note that we also getNaN values. These aredays that **don’t have****data**|ID|Date|Amount||---|---|---||1|Mon Apr 17 20:30 2023|5.24||2|Mon Apr 17 20:31 2023|10.98||3|Mon Apr 17 20:35 2023|1.58||4|Mon Apr 17 23:35 2023|50.60||5|Tues Apr 18 04:20<br>2023|34.01||6|Tues Apr 18 07:50<br>2023|25.05||7|Thurs Apr 20 12:50<br>2023|5.63|Just like with **groupby**, we can add a method to calculate some **summary****statistic** . This will select all rows that fall on **the same day…****df.resample(“1D”, on=”Date”)[“Amount”].mean()**Note that we also getNaN values. These aredays that **don’t have****data**|Date|Amount||---|---||Mon Apr 17 2023|17.1||Tues Apr 18 2023|29.53||Wednesday Apr 19 2023|NaN||Thurs Apr 20 2023|5.63|##### **df[\"date\"] = pd.to_datetime(df[\"date\"])** **df.resample(on=\"date\", rule=\"1D\")[\"sentiment\"].mean()**Let’s apply these same principles to our Twitter dataset to better visualizeour former line plot. First we will convert our “date” column to a date-timetype.##### **df[\"date\"] = pd.to_datetime(df[\"date\"])** **df.resample(on=\"date\", rule=\"1D\")[\"sentiment\"].mean()**Now we can resample our dataframe to calculate the average sentimentspers day. **While this is a good start**, how can we plot these values using aline plot?##### **df[\"date\"] = pd.to_datetime(df[\"date\"])** **df.resample(on=\"date\", rule=\"1D\")[\"sentiment\"].mean().plot.line()**###### **df.resample(on=\"date\", rule=\"1D\")[\"sentiment\"].mean().interpolate().plot.line()**## **Regex**As this is a Twitter dataset, we should probably utilize some sort oftext-based analysis. What have we learned about in the past which will helpus figure out what people are talking about?#### **Regular Expressions (Regex)****review**_Farukh is great__Where is Farukh?__Farrrrukh is terrible__I like Python27__@@@19586_|Regex Pattern|Col2||---|---||||_Farukh_**review**_Farukh is great__Where is Farukh?__Farrrrukh is terrible__I like Python27__@@@19586_For example, by just using the regex string “Farukh”, we will look for all rowsthat contain the string “Farukh.” Which rows will be matched?|Regex Pattern|Col2||---|---||||_Farukh_|Regex Pattern|Col2||---|---||||Remember, acomputer does notunderstand intent. Itwill only do exactlywhat you want it to do|Regex Pattern|Col2||---|---||||_Farukh_**review**_Farukh is great__Where is Farukh?__Farrrrukh is terrible__I like Python27__@@@19586_|Regex Pattern<br>^Far*ukh|review||---|---||Regex Pattern<br>_^Far*ukh_|_Farukh is great_||Regex Pattern<br>_^Far*ukh_|_Where is Farukh?_||Regex Pattern<br>_^Far*ukh_|_Farrrrukh is terrible_||Regex Pattern<br>_^Far*ukh_|_I like Python27_||Regex Pattern<br>_^Far*ukh_|_@@@19586_|By placing a caret at the front, we only find reviews that that begin with theword “Farukh” with an arbitrary number of r’s/|Regex Pattern|Col2||---|---|||||Regex Pattern|Col2||---|---||||Knowing regex will save you **hours of work** .#### **Regular Expressions (Regex)**Find all participants wholive in states startingwith “New”|age|age_spouse|State|income||---|---|---|---||32|35|New York|30,000||48|47|New Mexico|70,000||21|NA|California|20,0000|Find all participants wholive in states startingwith “New”|age|age_spouse|State|income||---|---|---|---||32|35|New York|30,000||48|47|New Mexico|70,000||21|NA|California|20,0000|Find all participants wholive in states startingwith “New”### **df[df.State.str.contains(“^New”)]**|age|age_spouse|State|income||---|---|---|---||32|35|New York|30,000||48|47|New Mexico|70,000||21|NA|California|20,0000||user|tweet||---|---||Daniiej|omg i\\'ve an economics test.||sensuoushel<br>p|FOX and the contestants won\\'t go about it<br>right||Taj_Milahi|good luck bro on the test||user|tweet||---|---||Daniiej|omg i\\'ve an economics test.||sensuoushel<br>p|FOX and the contestants won\\'t go about it<br>right||Taj_Milahi|good luck bro on the test||user|tweet||---|---||Daniiej|omg i\\'ve an economics test.||sensuoushel<br>p|FOX and the contestants won\\'t go about it<br>right||Taj_Milahi|good luck bro on the test|## **Twitter Dataset Case Study**Complete this analysis and meet back at 9:20 to answer analytical questionsvia the wheel.## **TLAB #3**',\n",
       " '# **Types of Visualizations** **Review**|“The existence of Comet NEOWISE (here depicted as a series|Col2||---|---||_of red dots) was discovered by analyzing astronomical survey_|_of red dots) was discovered by analyzing astronomical survey_||_data…”_||### **Agenda - Goals****●****Understand which visualizations to choose for your analysis****●****Learn how to make data visualizations in Python****●****Understand the fundamentals of matplotlib**## **Warm-Up**## **Review of Variables**### **Visualizing Data**### **Independent vs Dependent Variable**### **Quantitative vs Categorical Variables**Now that we’re equipped with these terms, which **variable is quantitative**and which **variable is categorical** ?Now that we’re equipped with these terms, which **variable is quantitative**and which **variable is categorical** ?Again, let’s continue to define these terms. Is our dependent variable**continuous** or **discrete** ?## **Visualizing Data****Univariate** **Bivariate** **Multivariate**### **Visualizing Data - Describing vs Prescribing**### **Visualizing Data - Descriptive Analytics**In descriptive analytics, we are:_**●**__**Exploring what occurred in the past**__**●**__**Identifying anomalies**__**●**__**Identifying relationships and patterns**_**Ex** :_Current users using our platform,__last-years sales, historical placement rates,__current flying conditions (wind, temp, etc)_### **Visualizing Data - Bar Graph**### **Visualizing Data - Histogram**### **Visualizing Data - Box Plot**### **Visualizing Data - More Visualizations**## **Creating Visualizations in** **Python**### **Matplotlib - General Form****import matplotlib.pyplot as plt****fig, ax = plt.subplots()****ax.plot([1,2,3,4,3,5,2,4])****ax.set_title(“Test Plot”)****ax.set_xlabel(“x-axis”)****ax.set_ylabel(“y-axis”)****fig.savefig(“test.png”)**Since matplotlib is not a built in package, you must first download it via pipand then import it to all subsequent code**ax.pie() # create pie chart****ax.bar() # create bar chart****ax.scatter() # create scatter plot****ax.set_ylim() # set y-axis range****fig.savefig() # save image****fig.show() # show image****fig.clear() # clear image**With this review of methods, let’s break down what these **axes** methods do.As well as what our **figure** method does. This creates a new image!Notice these are _almost_ the same as the implicit methods, except this timewe only use the **plt** package name to call of our methods.We can use the **bar()** plot to make bar graphs (2 lists, just like with ourline-plot)Often times when you do data analysis, you need to increase the number ofbins to capture capture the distribution of your dataset.Notice that by increasing the number of bins, the more “detail” we can see.At some point however, we get “diminishing” returns in understandability,**so don’t go too far here.****More polygons → More resolution**## **Common Matplotlib Problem**### **Common Matplotlib Problems**## **Wrap-Up**## **Glossary****population** - entire group you could possibly get data from**sample** - a subset of your population which you collect data from**feature** - an important component of your data**independent variable** - something that does not change when other features in your data change**dependent variable** - a value which changes when other features in your data change**discrete variable** - a variable which is measured in whole numbers (think integers)**continuous variable** - a variable which is measured with all real numbers, including decimals**categorical variable** - something that is describes things (e.g. color, type, class, etc;)**quantitative variable** - something that measures things (e.g. age, height, weight, etc;)**latent variable -** a variable that is not directly observable, but can be inferred from other variables that can bedirectly measured']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v=list(cleaned_dict.values())\n",
    "v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9f410b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 768)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "embeddings = embedder.encode(v, normalize_embeddings=True) \n",
    "\n",
    "embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b7aa5720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.01199111e-02,  5.79908155e-02,  5.79888709e-02,  1.21406307e-02,\n",
       "       -1.52504947e-02, -7.15315789e-02, -6.39610272e-03,  6.43311674e-03,\n",
       "       -2.06810255e-02, -3.04760803e-02, -5.48784398e-02,  3.08148079e-02,\n",
       "        1.46815153e-02,  1.76919736e-02, -6.36905134e-02,  5.42941839e-02,\n",
       "       -3.74312252e-02, -4.14067274e-03, -3.16684917e-02,  9.28819086e-03,\n",
       "       -3.86417564e-03, -3.47059965e-02, -3.86697091e-02,  2.26820428e-02,\n",
       "        5.29700667e-02,  7.79013708e-02, -7.52211642e-03, -3.25701642e-03,\n",
       "       -2.43004430e-02,  4.95911762e-02, -1.28615811e-03, -4.83563123e-03,\n",
       "        3.47193703e-02, -4.53156903e-02, -1.59777771e-03, -1.30981673e-02,\n",
       "        4.53111753e-02,  8.45846981e-02,  4.79580872e-02,  6.32581264e-02,\n",
       "        2.03281287e-02, -3.87657210e-02,  4.67089489e-02,  2.05880729e-03,\n",
       "       -7.98193887e-02, -4.50933762e-02,  2.99833156e-03,  6.45070150e-02,\n",
       "        4.38071452e-02, -3.75853628e-02,  2.88260467e-02,  3.31792468e-03,\n",
       "        7.14617372e-02, -3.32412310e-03, -3.00017726e-02, -3.18897888e-02,\n",
       "        2.22511180e-02, -9.44394246e-02, -3.76481898e-02,  4.09649638e-03,\n",
       "       -7.15108737e-02,  7.24138785e-03,  3.14874873e-02, -2.49963384e-02,\n",
       "       -4.36721072e-02,  5.13971448e-02,  4.08058204e-02, -1.35327661e-02,\n",
       "       -9.99038890e-02, -1.85593478e-02,  4.03659418e-02, -3.66589166e-02,\n",
       "       -3.83624099e-02,  1.04500875e-02, -1.58992987e-02, -2.24248692e-02,\n",
       "        2.24889535e-02, -7.59380695e-04,  6.09182194e-02,  2.65667215e-02,\n",
       "       -3.72288339e-02,  4.20926325e-03,  2.68932208e-02,  1.82142146e-02,\n",
       "        5.41767143e-02, -1.28829693e-02,  2.74318960e-02,  1.48380185e-02,\n",
       "       -2.95372382e-02,  2.29955502e-02,  2.56370027e-02,  8.73086508e-03,\n",
       "        1.04064150e-02, -3.05830855e-02, -4.91180494e-02, -2.21477039e-02,\n",
       "        3.29300538e-02,  4.45026555e-04, -1.51487738e-02,  2.67543718e-02,\n",
       "       -5.29311411e-02, -1.37547292e-02, -5.30712232e-02, -2.94353403e-02,\n",
       "       -3.67463715e-02,  3.19682434e-02, -2.68230750e-03, -4.29942086e-02,\n",
       "       -5.14842235e-02,  5.21903001e-02,  3.23864594e-02, -3.06740031e-02,\n",
       "       -4.09650318e-02,  2.17014290e-02, -4.66192625e-02, -2.47588679e-02,\n",
       "       -1.18789710e-02,  2.25319471e-02,  5.01673222e-02,  2.84117907e-02,\n",
       "       -3.29947956e-02,  3.89843583e-02, -4.08347063e-02,  6.30879914e-03,\n",
       "        1.40625779e-02,  2.45058853e-02, -1.53456051e-02,  7.04560652e-02,\n",
       "       -3.52243520e-02, -2.67902762e-03,  4.82945181e-02,  3.97573337e-02,\n",
       "        3.49329822e-02, -1.63138527e-02,  6.33601053e-03,  2.80559026e-02,\n",
       "       -3.10990792e-02, -3.87109555e-02, -2.15491075e-02, -3.38646360e-02,\n",
       "        1.18917916e-02, -4.16390002e-02,  2.15888321e-02,  1.33490423e-02,\n",
       "       -6.89337542e-03, -6.53886348e-02, -5.36324829e-02, -3.10609452e-02,\n",
       "        2.06650514e-03,  2.99173668e-02, -4.89030294e-02,  2.20343843e-02,\n",
       "       -9.53092277e-02, -4.05678861e-02,  7.44412001e-03, -1.07837599e-02,\n",
       "       -9.63504165e-02, -4.76282761e-02,  1.49256550e-02,  4.26736474e-02,\n",
       "        1.14357499e-02,  6.52351826e-02, -8.77334327e-02,  4.02947739e-02,\n",
       "        3.42849307e-02, -4.68162671e-02, -1.57214701e-02, -6.48935065e-02,\n",
       "       -1.72020122e-02,  2.88572870e-02, -1.46279414e-03,  2.24191998e-03,\n",
       "        2.68874876e-02,  5.62966568e-03,  5.90267032e-02,  6.81883516e-03,\n",
       "        2.85271723e-02, -4.68042009e-02, -4.35808040e-02, -2.69720070e-02,\n",
       "        5.94889605e-03, -2.65482683e-02, -6.49966151e-02,  3.14295292e-02,\n",
       "        3.19412574e-02, -2.76590623e-02,  5.19889109e-02, -6.20672619e-03,\n",
       "        4.17252667e-02, -6.43557357e-03, -6.99497247e-03, -1.96109265e-02,\n",
       "        3.09277419e-02, -1.04673384e-02,  6.94002444e-03,  7.84027204e-03,\n",
       "       -6.26114896e-03,  6.78247809e-02,  2.51940098e-02,  4.79667448e-02,\n",
       "        5.65359741e-03,  6.31084293e-03,  5.71125224e-02, -2.22705267e-02,\n",
       "       -2.97067873e-02,  3.79278436e-02, -2.64340267e-02,  3.97083461e-02,\n",
       "       -4.74710129e-02, -7.40389619e-03, -4.33383547e-02,  4.19141576e-02,\n",
       "        1.94243360e-02,  1.95702929e-02,  5.13130203e-02,  4.30757459e-03,\n",
       "        5.52873611e-02, -8.32820684e-03,  5.36743440e-02,  3.39301191e-02,\n",
       "        6.30075485e-02, -2.39066239e-02, -1.43521084e-02,  3.12610492e-02,\n",
       "       -3.81988683e-03,  7.54495040e-02,  2.88385432e-02, -9.92566347e-03,\n",
       "        1.91373024e-02,  3.99885960e-02,  3.67345139e-02, -2.22155526e-02,\n",
       "        7.71852629e-03,  8.53304286e-03,  3.11405063e-02,  1.65280001e-03,\n",
       "       -1.65259913e-02,  2.99680582e-03,  4.01773155e-02, -3.29468250e-02,\n",
       "        1.73037481e-02,  3.03115081e-02, -6.33458840e-03, -9.02303681e-03,\n",
       "        3.10201570e-03, -2.45899167e-02,  2.01356858e-02, -2.48506330e-02,\n",
       "       -2.28175279e-02,  2.65114792e-02, -3.87921520e-02,  2.08582357e-03,\n",
       "        5.60161471e-02, -7.24918619e-02,  4.51748259e-02, -2.51560286e-02,\n",
       "        2.74047554e-02,  1.34954331e-02, -4.77239713e-02,  2.21924055e-02,\n",
       "        1.13009317e-02, -4.06526439e-02, -6.10096417e-02,  2.40315795e-02,\n",
       "        1.79754775e-02, -2.69579664e-02,  2.48657726e-02,  6.40531108e-02,\n",
       "       -8.82382598e-03,  9.62817483e-03, -4.66833077e-02, -3.50622237e-02,\n",
       "       -3.53845768e-02,  3.07974359e-03, -4.82669175e-02, -4.68125641e-02,\n",
       "       -2.12735981e-02,  1.15797482e-02,  5.23927622e-02, -5.07669225e-02,\n",
       "       -6.13658801e-02, -2.13054148e-03, -2.50912569e-02,  1.68142747e-02,\n",
       "        3.45229805e-02,  1.55837527e-02,  2.13599764e-02, -5.50432503e-02,\n",
       "       -8.05427507e-02,  1.85429696e-02,  2.94000916e-02, -2.72456910e-02,\n",
       "       -4.11329651e-03, -1.02859223e-03,  1.47594037e-02,  5.73122548e-03,\n",
       "        4.94047925e-02, -2.68509891e-02,  2.26104800e-02,  4.72763972e-03,\n",
       "       -9.72658221e-04,  5.65011567e-03, -9.42806751e-02,  1.30840586e-02,\n",
       "       -1.08186454e-02, -6.06983118e-02,  3.95519892e-03, -2.05912087e-02,\n",
       "        1.23455552e-02, -2.31418777e-02, -2.76849400e-02, -4.01769159e-03,\n",
       "        2.34307908e-02,  6.76696002e-02,  2.53562517e-02,  1.15754567e-02,\n",
       "       -6.57708524e-03,  3.40430662e-02, -6.86336905e-02, -5.64640686e-02,\n",
       "        1.48150511e-03, -6.61580730e-03,  1.54596912e-02,  5.07205613e-02,\n",
       "        6.69745822e-03, -2.31734831e-02,  1.48860933e-02, -1.65611468e-02,\n",
       "       -2.47802921e-02, -1.98177155e-02,  6.13477733e-03, -2.10845334e-04,\n",
       "        3.25372554e-02,  6.13161847e-02, -4.50816229e-02,  4.25695954e-03,\n",
       "       -4.83041964e-02,  4.39790189e-02,  4.96539176e-02,  4.84298496e-03,\n",
       "       -1.47189032e-02,  3.03833373e-02, -1.81896556e-02, -2.23136209e-02,\n",
       "        2.66161282e-02, -3.54833640e-02, -6.91781892e-03,  2.47511938e-02,\n",
       "        9.20720957e-03,  3.51961404e-02, -1.88393332e-02, -7.58374110e-02,\n",
       "        6.63237050e-02,  1.83793884e-02, -3.99918631e-02, -4.16302830e-02,\n",
       "       -3.64771150e-02,  2.58680210e-02, -6.84251264e-02, -1.43839279e-02,\n",
       "       -8.08518939e-03,  8.50358084e-02, -1.80437267e-02, -5.02093062e-02,\n",
       "        2.10509612e-03,  5.89131601e-02, -3.70660610e-02, -4.09511477e-02,\n",
       "        1.90675016e-02,  1.82881225e-02,  2.19205283e-02, -5.04083466e-03,\n",
       "       -7.02966452e-02,  6.17530104e-03, -4.62940224e-02, -1.08226063e-02,\n",
       "       -3.96547019e-02, -3.62997279e-02, -8.09447560e-03,  4.87236716e-02,\n",
       "       -2.35766340e-02, -1.46657890e-02,  2.91682109e-02, -3.63434143e-02,\n",
       "       -1.00385742e-02,  4.01845090e-02,  4.08013910e-03,  1.22838587e-01,\n",
       "        3.35775726e-02, -1.96220372e-02, -1.58604458e-02,  9.75032151e-03,\n",
       "       -3.06298137e-02, -5.08758388e-02,  4.51367423e-02, -2.94987336e-02,\n",
       "        1.65623054e-02, -1.36027113e-02, -7.16090128e-02, -4.30551590e-03,\n",
       "       -3.58247720e-02, -1.52075780e-03,  9.25682932e-02, -4.11738828e-02,\n",
       "        2.41485238e-03, -7.37256557e-03,  4.30188999e-02, -1.57357696e-02,\n",
       "        2.43801028e-02, -4.28318977e-02,  1.27477916e-02, -6.16237484e-02,\n",
       "        3.21011320e-02,  9.15835425e-03, -1.07430127e-02,  1.79095753e-02,\n",
       "       -5.63010108e-03, -1.21304980e-02, -8.82279202e-02,  2.47060955e-02,\n",
       "       -6.51600361e-02, -6.86381338e-03,  5.00551537e-02,  1.18482590e-03,\n",
       "        3.49965282e-02,  3.01433494e-03, -4.35033627e-02, -8.48351326e-03,\n",
       "       -8.52555558e-02,  1.59702431e-02,  1.76883116e-02,  4.22322862e-02,\n",
       "       -1.28285224e-02, -3.20094675e-02,  9.25515406e-03, -3.63752693e-02,\n",
       "        6.10694699e-02,  6.01380190e-04,  3.14918309e-02,  2.89484765e-02,\n",
       "       -2.64538778e-03, -4.38718162e-02,  3.97746414e-02, -2.15910785e-02,\n",
       "       -1.12068970e-02, -3.17984959e-03,  2.31688865e-03,  3.47329862e-02,\n",
       "        2.00990532e-02, -6.08191127e-03,  2.90727075e-02, -4.84774821e-03,\n",
       "        5.52584836e-03,  2.55749375e-02,  9.11086798e-03,  3.75860371e-02,\n",
       "       -2.13225856e-02, -2.52454672e-02, -4.30688970e-02, -4.08653095e-02,\n",
       "        4.28046100e-02,  1.80821568e-02,  3.61684300e-02,  1.63919590e-02,\n",
       "        5.81842251e-02,  6.55213967e-02, -6.48737624e-02, -1.40182646e-02,\n",
       "        4.99433093e-02, -3.26230638e-02,  5.13213128e-02,  3.14126979e-03,\n",
       "       -8.72294605e-03,  7.97408894e-02, -4.55399863e-02, -2.70762984e-02,\n",
       "       -2.10409127e-02, -6.56381063e-03, -2.11622175e-02,  6.00254685e-02,\n",
       "       -4.91450401e-03,  1.24704698e-02,  2.25286651e-03,  2.38246433e-02,\n",
       "        2.52687931e-03,  1.73788518e-02, -8.79127067e-03,  3.31978202e-02,\n",
       "        8.82728305e-03, -3.19625475e-02, -4.07385267e-02,  1.49988690e-02,\n",
       "       -6.40241848e-03, -2.41123084e-02,  3.12345717e-02, -9.48595162e-03,\n",
       "       -1.05531365e-02, -1.03656529e-02,  3.92877311e-03, -1.72041263e-02,\n",
       "        8.12497213e-02,  4.65816036e-02,  2.58172750e-02, -4.80301864e-02,\n",
       "        8.19049105e-02,  2.74391137e-02, -5.91675118e-02,  6.34129858e-03,\n",
       "       -5.47137335e-02,  3.81328836e-02, -7.38103539e-02, -1.93738341e-02,\n",
       "        4.81096841e-02,  7.27817137e-03,  3.23912427e-02, -3.20039503e-03,\n",
       "        4.19926876e-03, -1.37424078e-02, -6.69611916e-02, -2.83232221e-04,\n",
       "       -1.93064697e-02, -4.26965095e-02,  4.40032892e-02, -8.33615139e-02,\n",
       "       -9.66397673e-03,  1.01852985e-02,  2.28348803e-02,  2.63728797e-02,\n",
       "        3.26868594e-02, -2.34908964e-02,  2.03931704e-02,  2.07241122e-02,\n",
       "       -8.21036752e-03,  2.51472350e-02, -5.73041216e-02,  2.42485683e-02,\n",
       "       -5.73461019e-02, -9.84092150e-03, -4.48514521e-03, -2.91242041e-02,\n",
       "        3.65500636e-02,  1.38929747e-02,  2.26750434e-03, -1.49015784e-02,\n",
       "       -5.23399301e-02, -1.94030050e-02,  2.08856240e-02,  8.07917863e-03,\n",
       "       -4.01518121e-02, -3.07450201e-02, -2.22045984e-02, -1.99610498e-02,\n",
       "        2.96979155e-02,  9.84493271e-03, -4.08847891e-02, -1.46319065e-03,\n",
       "       -4.99302261e-02,  7.69469794e-03,  8.73711985e-03, -2.17310302e-02,\n",
       "        2.91678663e-02,  3.11214123e-02, -3.26263048e-02,  2.15331968e-02,\n",
       "       -3.22231948e-02, -4.07638848e-02,  4.72527836e-03, -3.43630128e-02,\n",
       "       -7.62950629e-02,  3.83950444e-03,  2.45366059e-02, -2.57070083e-02,\n",
       "        3.64518771e-03,  7.72486776e-02, -8.91152844e-02, -4.40079272e-02,\n",
       "       -1.13471011e-02, -2.12716684e-02,  5.31937694e-03, -6.18044112e-04,\n",
       "       -2.40105782e-02,  2.52822731e-02, -3.84292230e-02,  2.75130905e-02,\n",
       "       -5.00807213e-03,  1.58996042e-02, -6.50740461e-03,  2.11811569e-02,\n",
       "       -7.44663924e-03, -2.52820533e-02, -9.56500228e-03,  4.18481159e-06,\n",
       "        1.16155454e-04, -1.57231316e-02,  2.06223107e-03, -1.33930119e-02,\n",
       "        2.01830063e-02,  3.14357989e-02,  3.77953649e-02,  1.97851029e-03,\n",
       "        1.02336463e-02, -1.89723875e-02, -4.25273478e-02,  1.49853975e-02,\n",
       "        2.75506340e-02,  3.40677798e-03, -2.00169277e-03,  3.53896990e-02,\n",
       "       -4.77509499e-02, -6.88232854e-03, -2.12698318e-02, -6.62481487e-02,\n",
       "       -5.34188887e-03,  1.68316043e-03, -7.45524606e-03,  7.65520195e-03,\n",
       "       -3.18697840e-02, -1.61708929e-02, -1.80620477e-02, -7.75939459e-03,\n",
       "        4.82177781e-03,  3.43820639e-02, -3.63199748e-02, -4.24790308e-02,\n",
       "        3.57568450e-02, -3.27770337e-02,  3.58273508e-03,  1.56403857e-03,\n",
       "       -2.91321576e-02, -1.11822458e-02,  3.58746946e-02,  2.15418581e-02,\n",
       "       -8.02556705e-03,  2.28763930e-02,  3.39382254e-02,  5.39060421e-02,\n",
       "       -4.13247347e-02,  3.47295851e-02,  3.39432573e-03,  2.26916596e-02,\n",
       "       -2.44295485e-02,  2.45176759e-02,  1.25208916e-02, -8.22778121e-02,\n",
       "        1.50729185e-02, -9.98166297e-03, -5.47748851e-03,  3.48183140e-02,\n",
       "       -3.76810320e-02,  5.29778711e-02, -1.58966857e-03,  5.46683222e-02,\n",
       "        1.50641249e-02,  1.78597420e-02, -1.84267983e-02,  3.53789106e-02,\n",
       "        2.65327133e-02, -2.19306611e-02,  6.91059697e-03,  4.84528430e-02,\n",
       "       -6.55087549e-03,  4.27525584e-03,  3.91608849e-02,  3.09133697e-02,\n",
       "        7.85019398e-02,  7.86855072e-03,  2.18667891e-02,  3.02154049e-02,\n",
       "        3.78663093e-02, -4.35519628e-02,  1.28818974e-02,  7.35305250e-03,\n",
       "       -2.01671254e-02, -2.60706954e-02,  7.32353255e-02, -1.54959811e-02,\n",
       "        1.88835207e-02, -2.82456893e-02,  1.88674200e-02, -8.28095824e-02,\n",
       "       -7.29760304e-02, -4.37419899e-02, -4.39268537e-02, -7.56198838e-02,\n",
       "        1.62256006e-02,  4.29009087e-02, -3.43441330e-02,  6.68707816e-03,\n",
       "        5.34251742e-02,  9.90119353e-02,  2.94532347e-02, -2.91050002e-02,\n",
       "        2.01900601e-02,  7.47467857e-03, -2.07094345e-02, -1.63159135e-03,\n",
       "       -2.62293586e-04,  4.04647812e-02,  1.92472599e-02, -5.78107825e-03,\n",
       "       -1.75511856e-02,  5.35984188e-02,  6.70336559e-03, -4.30596471e-02,\n",
       "        1.58901494e-02,  4.18049730e-02,  4.15984131e-02, -3.70265767e-02,\n",
       "       -5.16457930e-02, -2.17858367e-02,  1.95014663e-02,  4.40504663e-02,\n",
       "        6.27809092e-02,  1.33528132e-02, -6.57661408e-02, -6.91113696e-02,\n",
       "        1.15935290e-02,  4.15651202e-02,  1.14170676e-02,  3.54875363e-02,\n",
       "       -6.62799599e-03,  3.16270664e-02, -4.00736891e-02, -5.54455332e-02,\n",
       "        7.43385125e-03, -3.24941315e-02,  3.51529010e-03, -1.93491764e-02,\n",
       "        1.41955717e-02, -3.84767391e-02,  5.50613776e-02, -3.78730260e-02,\n",
       "        3.54474806e-03,  3.80369723e-02, -5.34376735e-03, -3.84527980e-03,\n",
       "       -4.82890150e-03, -2.31261458e-02, -6.46207761e-03, -4.52378504e-02,\n",
       "        3.59696038e-02, -4.41477560e-02, -5.53413481e-02, -1.50651047e-02,\n",
       "        9.60742217e-03,  1.82113647e-02, -2.53788661e-02,  2.45936657e-03,\n",
       "       -1.70268733e-02,  4.97980835e-03, -3.15394811e-02, -7.51275718e-02,\n",
       "        1.31277181e-02,  8.59513432e-02, -1.38983261e-02,  2.78725917e-03,\n",
       "       -5.56046106e-02, -3.57736424e-02, -7.04389736e-02, -6.04411326e-02,\n",
       "       -9.62870345e-02, -6.64142668e-02, -2.72959750e-02, -3.58605869e-02,\n",
       "       -1.04999924e-02,  4.13042232e-02, -9.23620164e-03,  2.33521070e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aaa02298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.80418212e-02,  1.17017096e-02,  1.21868044e-01,\n",
       "         1.48525666e-02,  6.66521117e-03,  2.42814180e-02,\n",
       "         8.00762326e-03,  5.16308285e-03,  3.15215476e-02,\n",
       "        -3.21275592e-02, -4.03966159e-02,  1.54438298e-02,\n",
       "         3.91624123e-02,  3.04323509e-02, -4.09279801e-02,\n",
       "         7.29615148e-03, -1.62305199e-02,  1.64745785e-02,\n",
       "         9.52084456e-03, -4.96597365e-02, -3.11849115e-04,\n",
       "         3.46350968e-02, -8.93158093e-03, -3.42765152e-02,\n",
       "         3.62617187e-02,  1.48761244e-02,  7.05586467e-03,\n",
       "        -1.21420883e-01,  7.26143830e-03,  3.28960493e-02,\n",
       "         5.20585217e-02,  4.83457819e-02, -1.13796408e-03,\n",
       "         5.95010526e-04, -2.38391850e-03,  4.32989839e-03,\n",
       "         1.19020911e-02,  1.45471087e-02,  1.58805065e-02,\n",
       "        -1.24188233e-03,  9.95428711e-02, -1.79950390e-02,\n",
       "         5.35396077e-02,  3.08162626e-02, -7.41191208e-02,\n",
       "        -3.52565423e-02, -4.00094315e-02,  3.93323340e-02,\n",
       "         2.14874186e-02, -8.60020518e-02, -4.25517783e-02,\n",
       "        -2.40816064e-02,  2.03200597e-02,  4.49473970e-02,\n",
       "        -7.25665987e-02,  7.97943212e-03,  6.24218062e-02,\n",
       "        -7.24467784e-02,  3.90411280e-02,  5.25948452e-03,\n",
       "        -2.36554425e-02, -2.68544070e-02,  6.29188344e-02,\n",
       "         6.77913874e-02, -4.70085740e-02,  5.14615625e-02,\n",
       "         6.74311072e-02, -2.35571582e-02, -1.15937646e-02,\n",
       "         2.90828142e-02,  2.48225015e-02,  3.34760919e-02,\n",
       "         5.56393852e-03,  5.12859225e-02, -5.34817018e-02,\n",
       "        -2.48364843e-02, -3.58385481e-02,  2.32940912e-02,\n",
       "         8.77961516e-02,  2.63596978e-02, -7.02625141e-03,\n",
       "         7.92251714e-03,  4.69564274e-02,  8.50013830e-03,\n",
       "         2.94966400e-02,  1.48499403e-02,  5.09668328e-02,\n",
       "        -2.97111310e-02, -1.38168037e-01,  3.67710590e-02,\n",
       "         2.76475511e-02,  4.60115867e-03,  8.04208145e-02,\n",
       "         2.61740219e-02, -1.52109424e-02,  1.92689197e-03,\n",
       "         4.38752249e-02, -1.74055845e-02,  5.00790030e-02,\n",
       "        -2.95592993e-02, -2.37610415e-02, -3.53551619e-02,\n",
       "        -2.68538413e-03,  7.35343294e-03, -4.76262495e-02,\n",
       "        -7.27404328e-03,  1.08791059e-02, -2.25236230e-02,\n",
       "         9.07660648e-03, -2.91033625e-03, -1.75566189e-02,\n",
       "        -7.55362166e-03,  4.50336598e-02,  5.61363064e-02,\n",
       "        -3.97202857e-02, -5.67796035e-03, -5.60782515e-02,\n",
       "         7.30090030e-03, -3.73620279e-02,  4.88990769e-02,\n",
       "        -4.48939353e-02,  7.35916272e-02, -4.07674387e-02,\n",
       "        -1.37616098e-02,  5.60844839e-02,  7.35978410e-03,\n",
       "         5.75717632e-03,  3.06718275e-02, -8.62258524e-02,\n",
       "         1.36959655e-02,  5.98072726e-03,  2.13198848e-02,\n",
       "         2.99223624e-02, -1.11896573e-02, -1.05469562e-02,\n",
       "         4.85052558e-04, -4.00898792e-02,  4.69364133e-03,\n",
       "        -1.97450574e-02,  1.18289459e-02, -1.23211741e-02,\n",
       "        -1.95010882e-02,  2.69410331e-02,  2.55002733e-02,\n",
       "         2.12935507e-02, -3.34694870e-02,  6.91206753e-03,\n",
       "        -3.14366221e-02,  1.64355505e-02,  4.60706502e-02,\n",
       "        -3.00912820e-02,  6.31188005e-02, -4.47080657e-02,\n",
       "        -3.69438529e-02, -4.52894457e-02, -1.53159909e-02,\n",
       "        -3.61093357e-02,  1.93484034e-02, -6.05549570e-03,\n",
       "         2.48426031e-02, -1.27620976e-02,  6.60128742e-02,\n",
       "         2.46587303e-02, -3.53431813e-02, -1.64581407e-02,\n",
       "        -7.78278429e-03,  5.47209848e-03, -4.27014835e-04,\n",
       "        -1.26817089e-03,  1.33964866e-02,  5.43924868e-02,\n",
       "        -5.25045618e-02,  4.87135462e-02,  4.71371263e-02,\n",
       "         4.61318009e-02, -3.82828154e-02, -4.21822956e-03,\n",
       "        -1.81654003e-02, -8.01572669e-03, -3.14700641e-02,\n",
       "         9.94153507e-03,  3.68161574e-02, -1.21371821e-02,\n",
       "        -2.30268873e-02, -5.29921763e-02, -1.71663426e-02,\n",
       "        -7.08990768e-02,  7.86195602e-03,  2.96714660e-02,\n",
       "        -3.83353420e-02, -3.16847768e-03, -4.26148102e-02,\n",
       "        -5.33100478e-02, -5.18267974e-02,  2.70466041e-02,\n",
       "         2.98534129e-02,  4.55662161e-02, -4.87881107e-03,\n",
       "        -6.94291480e-03, -2.24473793e-02,  8.41809660e-02,\n",
       "        -6.02866262e-02,  6.59617828e-03,  3.27681154e-02,\n",
       "         9.74438339e-03,  6.05995469e-02,  2.97889370e-03,\n",
       "         8.40907078e-03,  1.17561137e-02, -3.57771330e-02,\n",
       "        -2.49739904e-02,  4.63719517e-02,  4.18195315e-02,\n",
       "         8.56801867e-03, -6.97782589e-03, -1.71278026e-02,\n",
       "         6.52118102e-02, -1.15190987e-02, -2.12335512e-02,\n",
       "         1.93653014e-02,  5.42653762e-02,  6.51763007e-03,\n",
       "         4.68379520e-02,  2.75514238e-02, -4.12778445e-02,\n",
       "         2.89993323e-02,  5.38441241e-02, -6.38542771e-02,\n",
       "        -6.43653274e-02,  1.65068135e-02,  3.41635942e-02,\n",
       "        -1.61089040e-02, -5.21721058e-02,  4.87458939e-03,\n",
       "        -1.05549628e-02,  1.32537670e-02, -2.73685828e-02,\n",
       "         1.75991710e-02,  5.60200475e-02, -2.34081596e-02,\n",
       "        -1.00334939e-02,  3.85520197e-02,  3.74176279e-02,\n",
       "         1.10931182e-02, -6.11437485e-02, -3.45459618e-02,\n",
       "         3.44007984e-02, -6.60888851e-02, -5.00736870e-02,\n",
       "         3.54417749e-02, -3.04968022e-02, -3.73396575e-02,\n",
       "         1.32644577e-02,  2.17830157e-03,  1.36155449e-02,\n",
       "         5.54128084e-03,  2.37005600e-03, -1.79186910e-02,\n",
       "         2.88277175e-02, -2.81657781e-02,  2.71490449e-03,\n",
       "        -3.01728826e-02, -1.46990800e-02, -4.67949659e-02,\n",
       "        -3.50197963e-03,  2.24008784e-02,  7.05056777e-03,\n",
       "        -3.36507894e-03,  1.60031542e-02,  7.02702627e-03,\n",
       "        -2.78567187e-02,  1.56247104e-02,  3.36558674e-03,\n",
       "        -1.93904992e-02, -2.62023024e-02, -1.87504441e-02,\n",
       "         6.65491000e-02,  3.02142818e-02, -1.67219099e-02,\n",
       "        -1.40586775e-02, -2.42637862e-02,  3.64268832e-02,\n",
       "        -1.76474378e-01, -2.57075634e-02,  4.32567336e-02,\n",
       "        -3.09071634e-02,  1.89058185e-02, -4.20731641e-02,\n",
       "        -1.45210410e-02, -1.71841606e-02, -2.77895834e-02,\n",
       "         8.85355566e-03,  4.08457592e-03, -7.57972673e-02,\n",
       "         4.22197916e-02,  4.32537682e-02,  6.06065094e-02,\n",
       "        -2.48632878e-02, -4.32169773e-02, -8.57202709e-03,\n",
       "        -5.44678494e-02, -3.40819284e-02, -3.98699902e-02,\n",
       "        -4.07166360e-03,  1.04271146e-02, -2.94784363e-02,\n",
       "         8.39770585e-03,  7.77346315e-04, -9.33114439e-02,\n",
       "        -8.01394694e-03, -8.84965137e-02, -7.01932516e-03,\n",
       "         2.78643686e-02,  1.35915373e-02,  6.72932155e-03,\n",
       "         2.20993198e-02, -1.00462874e-02,  5.32142706e-02,\n",
       "         4.87045459e-02,  2.18916871e-02, -3.52154523e-02,\n",
       "        -3.67297558e-03,  2.18177168e-03, -1.41650289e-02,\n",
       "         1.01356243e-03,  3.92882749e-02, -2.07127687e-02,\n",
       "        -2.00490355e-02, -4.07045409e-02,  3.11930198e-02,\n",
       "         4.40651067e-02,  4.10043215e-03, -9.10283253e-03,\n",
       "         8.04424100e-03,  1.20341936e-02,  4.48238887e-02,\n",
       "        -1.82357728e-02, -6.68730661e-02, -1.34932175e-02,\n",
       "         3.58700305e-02,  4.09729630e-02,  2.85801664e-03,\n",
       "        -2.15976080e-03,  1.56373866e-02,  1.69002339e-02,\n",
       "        -2.40331348e-02, -1.07417500e-03, -1.18645476e-02,\n",
       "         2.66859178e-02,  9.74688902e-02,  4.08653617e-02,\n",
       "         5.09189675e-03,  3.77156883e-02,  1.18879124e-03,\n",
       "        -1.69734284e-02, -1.77853126e-02,  4.31727152e-03,\n",
       "         8.66339263e-03, -3.55581231e-02,  2.81086266e-02,\n",
       "        -1.67274140e-02,  5.66913299e-02, -6.17239550e-02,\n",
       "        -6.28030971e-02, -2.33931411e-02, -5.09736463e-02,\n",
       "        -6.46951026e-04, -8.96714535e-03,  7.65288621e-02,\n",
       "         7.36049470e-03,  7.13652233e-03, -5.12926839e-02,\n",
       "        -4.44885418e-02, -1.42132901e-02, -3.85042541e-02,\n",
       "        -1.23305256e-02, -1.03124287e-02, -8.03115517e-02,\n",
       "        -1.27113806e-02, -3.88355777e-02,  7.20780343e-02,\n",
       "        -5.18464334e-02, -2.35378332e-02,  8.38404428e-03,\n",
       "         1.24411769e-02,  2.17210259e-02,  5.35988156e-03,\n",
       "         8.39852728e-03,  3.93437222e-02, -2.18314752e-02,\n",
       "         3.25155482e-02,  2.96560712e-02, -1.40570465e-03,\n",
       "        -4.66008112e-03, -1.17582856e-02, -5.23675047e-02,\n",
       "         4.95936610e-02,  3.11430637e-02, -5.91857126e-03,\n",
       "        -2.03503724e-02, -7.72666710e-04, -6.47723954e-03,\n",
       "         7.26729706e-02,  5.28720068e-03,  5.10051940e-03,\n",
       "        -4.61054221e-02,  3.01550459e-02,  7.53823714e-03,\n",
       "        -2.12863088e-02,  2.21237913e-02, -3.97147350e-02,\n",
       "         1.25579908e-02,  1.41426064e-02,  5.66148292e-03,\n",
       "        -3.32948640e-02,  2.58623976e-02,  1.45453047e-02,\n",
       "        -8.01455975e-02,  8.42687488e-03, -2.57937741e-02,\n",
       "        -1.47586493e-02, -3.13735679e-02, -2.87588164e-02,\n",
       "        -6.84734285e-02, -5.24216667e-02, -8.01236369e-03,\n",
       "        -2.80974880e-02,  2.07081810e-02, -1.90581288e-02,\n",
       "        -5.42860292e-02, -1.71078518e-02,  1.77461542e-02,\n",
       "        -1.14244975e-01, -9.03779361e-03, -2.21591275e-02,\n",
       "        -6.21534586e-02,  2.64214631e-03,  1.53991282e-02,\n",
       "         2.74154693e-02,  2.96186302e-02,  6.44053593e-02,\n",
       "        -4.95592551e-03, -8.09807051e-03,  2.17163209e-02,\n",
       "         1.50138503e-02, -2.83418898e-03, -2.36803573e-02,\n",
       "        -6.49709553e-02,  6.70130784e-03, -3.46397497e-02,\n",
       "        -4.84686811e-03,  4.51077819e-02,  2.24759802e-02,\n",
       "        -1.43692521e-02,  2.55235229e-02,  5.84196718e-03,\n",
       "        -3.61484252e-02,  2.85937591e-03,  5.02637215e-03,\n",
       "        -2.99432557e-02,  2.48913132e-02,  7.99740432e-04,\n",
       "        -6.31686999e-03,  1.83587968e-02,  3.55723910e-02,\n",
       "         5.91543596e-03, -4.74295802e-02,  4.14591879e-02,\n",
       "        -1.31671028e-02,  2.22439021e-02,  4.49495688e-02,\n",
       "         2.64622550e-02,  3.29989195e-02, -1.30622564e-02,\n",
       "         2.59006545e-02,  2.98743937e-02, -8.43182653e-02,\n",
       "         3.95626053e-02,  4.73138653e-02,  4.04859073e-02,\n",
       "        -1.91158000e-02,  2.24334765e-02, -3.55731063e-02,\n",
       "         1.03184832e-02, -9.56877694e-03,  6.50185421e-02,\n",
       "         1.59989018e-02, -3.74196619e-02,  7.48363510e-02,\n",
       "         1.13665278e-03, -6.75357282e-02, -2.80156285e-02,\n",
       "        -3.04416846e-02, -4.11496162e-02,  3.38402279e-02,\n",
       "        -5.32703176e-02,  3.96833569e-03, -3.21071111e-02,\n",
       "        -4.57817642e-03, -5.46339378e-02, -7.47951446e-03,\n",
       "         3.44452858e-02,  5.15361428e-02, -3.50709073e-02,\n",
       "         2.52117030e-02, -3.30145005e-03, -6.12023920e-02,\n",
       "         4.21103574e-02, -2.39134301e-02,  4.96510975e-02,\n",
       "        -5.18277772e-02,  2.72105895e-02,  1.30592557e-02,\n",
       "        -3.19053903e-02,  2.65341233e-02, -4.58056331e-02,\n",
       "        -1.24988686e-02, -5.03340326e-02, -9.33726504e-03,\n",
       "        -5.82622066e-02, -2.07959693e-02,  4.91712103e-03,\n",
       "         5.57358898e-02, -1.75805893e-02,  1.77504029e-02,\n",
       "        -2.08256836e-03,  3.97340953e-02, -3.62032987e-02,\n",
       "         3.67664024e-02, -1.97468083e-02, -2.32782308e-02,\n",
       "        -6.29109796e-03, -1.90246683e-02,  2.04775464e-02,\n",
       "        -4.57535461e-02,  3.20908688e-02, -3.10518946e-02,\n",
       "        -5.63197695e-02,  3.21307629e-02, -9.77486465e-03,\n",
       "         3.06313243e-02,  2.10141353e-02,  2.55156588e-02,\n",
       "        -1.73404219e-03, -8.09726119e-02, -2.26227008e-02,\n",
       "         8.16108150e-05, -3.05637009e-02, -8.36975966e-03,\n",
       "        -3.83260101e-02,  3.67407850e-03, -4.14194912e-03,\n",
       "        -5.64373285e-02, -2.78117228e-02, -4.23764735e-02,\n",
       "         2.19813343e-02, -1.37592023e-02,  1.71796698e-02,\n",
       "        -7.33479764e-03, -2.85403132e-02,  1.70475394e-02,\n",
       "         2.47103535e-02, -2.09692623e-02,  4.60373424e-02,\n",
       "        -3.55161056e-02, -3.12924497e-02,  3.71812959e-03,\n",
       "        -4.35674610e-03, -2.18264237e-02, -7.63603626e-03,\n",
       "         1.62933450e-02, -3.07560861e-02, -1.02967257e-02,\n",
       "         4.15613167e-02, -4.13035564e-02,  1.38739664e-02,\n",
       "        -6.92475028e-03, -2.59215303e-04, -8.90207216e-02,\n",
       "        -6.47160457e-03, -5.82051743e-03,  3.02117690e-02,\n",
       "         1.64157897e-02, -2.52849776e-02, -1.89249106e-02,\n",
       "         8.22804049e-02, -6.81732967e-02, -3.00206691e-02,\n",
       "         9.31999180e-03,  3.32677290e-02,  3.89744225e-03,\n",
       "        -1.02886967e-02, -2.88223871e-03, -2.94706784e-02,\n",
       "         1.68898664e-02, -6.77207811e-03,  4.34058011e-02,\n",
       "         3.57006229e-02, -1.69653166e-02,  2.86432654e-02,\n",
       "         2.47095432e-02,  4.98051532e-02, -2.29436196e-02,\n",
       "         3.01612057e-02, -1.75323971e-02,  7.10210279e-02,\n",
       "         9.14929435e-03,  3.67652811e-02, -2.06625070e-02,\n",
       "         3.33077759e-02, -4.10440890e-03, -6.03027083e-02,\n",
       "        -9.67063475e-03,  3.81205082e-02, -6.48969377e-04,\n",
       "        -3.17789167e-02, -1.61298588e-02,  2.29888447e-02,\n",
       "         2.24513263e-02,  1.89738367e-02,  1.60760116e-02,\n",
       "         3.59339938e-02, -2.20053480e-04,  1.84594337e-02,\n",
       "        -2.22093854e-02, -6.18458465e-02,  1.29839005e-02,\n",
       "        -2.02965252e-02, -4.65211868e-02,  2.74218316e-03,\n",
       "         3.84650007e-02,  3.48995663e-02,  2.35837661e-02,\n",
       "         1.03443917e-02, -1.15526067e-02,  4.18710150e-02,\n",
       "        -2.11883020e-02,  4.48284782e-02, -7.98256323e-03,\n",
       "        -1.59802893e-03,  9.30733699e-03, -5.13347145e-03,\n",
       "         1.24823591e-02,  6.13596872e-04, -3.16238031e-02,\n",
       "         3.86398397e-02, -4.70114406e-03,  7.11079240e-02,\n",
       "         1.38316853e-02,  4.45268452e-02,  2.02324130e-02,\n",
       "        -2.91755199e-02, -2.93515902e-02, -1.62939932e-02,\n",
       "         1.17556201e-02, -3.19418646e-02,  1.80795975e-02,\n",
       "        -8.74988455e-03,  2.62808204e-02, -3.14811580e-02,\n",
       "         1.83712859e-02,  6.76787598e-03,  9.14665498e-03,\n",
       "        -7.78149487e-03,  3.93049791e-02,  1.76762827e-02,\n",
       "         1.42418481e-02, -2.91983038e-02,  1.33167151e-02,\n",
       "        -5.62132001e-02,  3.86102721e-02, -2.09987368e-02,\n",
       "         9.46357381e-03, -1.47593454e-01,  5.10905962e-03,\n",
       "        -2.66089179e-02,  6.20831996e-02,  3.24173942e-02,\n",
       "        -5.96500486e-02,  4.45740623e-03,  5.98903522e-02,\n",
       "         3.68733369e-02,  7.67817255e-03, -1.99973453e-02,\n",
       "        -4.94344831e-02, -2.81922705e-02, -5.86093683e-03,\n",
       "        -1.34567702e-02,  1.81779582e-02, -5.15971407e-02,\n",
       "         1.89889353e-02,  2.95796767e-02, -6.17506634e-03,\n",
       "        -8.49692598e-02, -6.05666079e-03,  2.71460563e-02,\n",
       "        -3.10404599e-02, -2.63862610e-02,  6.38882890e-02,\n",
       "        -3.64528745e-02, -3.99863869e-02,  2.93953642e-02,\n",
       "        -7.48920292e-02,  4.98011820e-02,  9.87682841e-04,\n",
       "         2.19217385e-03, -3.81560810e-02,  1.04222028e-02,\n",
       "        -1.58349536e-02,  1.33452751e-03,  2.30789254e-03,\n",
       "         1.94921624e-02,  7.48483688e-02, -1.92884647e-03,\n",
       "        -3.92633192e-02,  8.11192095e-02,  4.96205613e-02,\n",
       "        -3.94322462e-02,  3.01112328e-02, -7.37449452e-02,\n",
       "        -1.21565107e-02,  2.59684981e-04,  1.18945707e-02,\n",
       "         3.72520946e-02, -1.03935599e-02, -1.14021811e-03,\n",
       "        -1.67300049e-02,  2.95813531e-02,  3.84522090e-03,\n",
       "        -4.72099148e-02,  2.41897553e-02,  3.08479212e-04,\n",
       "        -2.18417100e-03,  2.21642535e-02,  2.11234614e-02,\n",
       "         1.61357578e-02, -1.70634743e-02, -1.49190603e-02,\n",
       "        -2.13538446e-02, -5.04331291e-03, -1.97319947e-02,\n",
       "         5.21622074e-04, -1.94899421e-02, -4.86968271e-02,\n",
       "        -2.45638359e-02, -3.01871188e-02,  1.13199241e-02,\n",
       "         3.85585264e-03, -2.57933252e-02, -1.46805225e-02,\n",
       "         8.29235986e-02,  9.09564942e-02, -1.80751849e-02,\n",
       "         2.56999601e-02, -4.25440818e-02,  2.48963619e-03,\n",
       "        -8.18562433e-02,  7.51428888e-04,  1.09313866e-02,\n",
       "        -3.84127684e-02,  4.11799774e-02, -4.32674736e-02,\n",
       "         1.62509549e-03, -1.05730910e-02, -1.44657437e-02,\n",
       "        -2.25455556e-02, -3.10899280e-02,  1.42342849e-02]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prompt = \"K-Nearest Neighbors is \"\n",
    "\n",
    "emb_prompt = embedder.encode([test_prompt], normalize_embeddings=True)\n",
    "emb_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "182e18a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss_avx2.IndexFlatIP; proxy of <Swig Object of type 'faiss::IndexFlatIP *' at 0x0000027704569B00> >"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f5f08fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.28532934 0.27235723 0.26590708]]\n",
      "[[3 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# use FAISS to get top 3 similar vectors\n",
    "top_k = 3\n",
    "scores, ids = index.search(emb_prompt, top_k)\n",
    "print(scores)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50b0b84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7f284e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa7c72e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d819b52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345e2e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca8e33c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
